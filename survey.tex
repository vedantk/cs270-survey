\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage[a4paper]{geometry}

\title{Spectral Graph Sparsification}
\author{\small{Vedant Kumar -- \texttt{vsk@berkeley.edu} -- CS270 Survey}}

\begin{document}
\maketitle

\newcommand \cut[1]{\text{cut}_{#1}}
\newcommand \textlcsc[1]{\textsc{\MakeLowercase{#1}}}
\newcommand \Tr{\text{Tr}}

\section*{Abstract}

Graph sparsification algorithms find sparse approximations of graphs. In
this paper we survey major results in spectral sparsification, a special
case in which important structural and algebraic properties of graphs are
preserved. We explain a key result: that every graph has a spectral
sparsifier with $O(|V|\log|V|)$ edges w.h.p, and that such sparsifiers can
be found in $\tilde{O}(|E|)$ time. We finish by discussing some applications
of spectral sparsification, such as computing electrical flows and finding
min-cuts in nearly-linear time.

\section{Introduction}

Sparse graphs have a similar number of edges and vertices, up to a polylog
factor ($|E| \in \tilde{O}(|V|)$). In contrast, dense graphs (such as the
complete graph) permit a quadratic number of edges ($|E| \in O(|V|^2)$). If
the number of vertices is fixed, many graph algorithms run significantly
faster and with lower storage requirements on sparse inputs.  Moreover, each
edge in a sparse graph tends to contribute `more' to the overall shape and
structure of the graph.  Working with sparse graphs can save time, save
space, and provide insight into the nature of a graph: this makes graph
sparsification interesting.

In order to assess the quality of a sparsification algorithm, we need a way
to measure similarity between a graph and its sparse approximation. Section
2 discusses two different measures of graph similarity -- \textit{cut
similarity} and \textit{spectral similarity} -- both of which led to the
discovery of important sparsification algorithms. Section 3 contains a
detailed exposition of a fast spectral sparsification algorithm based on an
edge-sampling scheme. Section 4 mentions some applications.

\section{Graph Similarity}

In this section we explain how \textit{cut similarity} naturally leads to
\textit{spectral similarity}, a strictly stronger criterion for measuring
the quality of a sparsifier. 

\subsection{Conventions}

The only graphs we consider are undirected, weighted, and connected. A graph
can be described by a tuple $G = (V, E, w)$, where $w : V^2 \rightarrow \Re$
maps edges to their weights. All edges must have positive weights, so $w(u,
v) = 0 \Leftrightarrow (u, v) \not\in E$, allowing us to write $G = (V, w)$
w.l.o.g. For notational simplicity we define $n = |V|$ and $m = |E|$. When
comparing two graphs, we always assume that they share the same vertex set.

\subsection{Cut similarity}

Two graphs are \textit{cut similar} if all of their cuts have approximately
similar weights. To make this concrete, consider two graphs $G = (V, w)$ and
$\tilde{G} = (V, \tilde{w})$. A \textit{cut} is a subset of vertices $S
\subset V$.  The weight of a cut is given by the sum of the weights of edges
on the cut:
\begin{align*}
    \cut{G}(S) = \sum_{u \in S, v \in V - S} w(u, v)
\end{align*}
$G$ and $\tilde{G}$ are $\epsilon$-cut similar if for all possible cuts, we
have:
\begin{align*}
    (1 - \epsilon)\cut{\tilde{G}}(S) \leq \cut{G}(S) \leq (1 +
    \epsilon)\cut{\tilde{G}}(S) \quad \forall{S \subset V}
\end{align*}
An important early result states that every graph has a good cut similar
approximation with $\tilde{O}(n)$ edges, and that this approximation can be
found quickly: \\

\noindent
\textlcsc{Theorem 1 (Bencz\'{u}r-Karger)}: \textit{Let $\epsilon > 0$. $G =
(V, E)$ has an $\epsilon$-cut similar graph $\tilde{G} \subset G$ s.t
$\tilde{m} \in O(\epsilon^{-2}n\log n)$. $\tilde{G}$ can be found in
$O(m\log^3n + \epsilon^{-2}m\log n)$ time \cite{BenczurKarger}.} \\

This version of Theorem 1 is taken from a later survey \cite{TheSurvey}. The
original paper claims that an even better $O(m\log^2 n)$ time bound is
possible using the union-find data structure. Regardless, this result sets
the stage for future work by showing that high-quality cut-similar
sparsifiers exist for \textit{every} graph. 

\subsection{Spectral similarity}

The \textit{Laplacian quadratic form} of a graph $G = (V, E, w)$ is given by
the map $Q_G : \Re^n \rightarrow \Re$:
\begin{align*}
    Q_G(x) = \sum_{(u, v) \in E} w(u, v)(x(u) - x(v))^2
\end{align*}
Consider a cut $S \subset V$: the $i$-th component of its
\textit{characteristic vector} $\mathbf{1}_S$ is 1 if $v_i \in S$, and is 0
otherwise. The expression $(\mathbf{1}_S(u) - \mathbf{1}_S(v))^2$ is 1 iff
$(u, v)$ is an edge on the cut boundary of $S$, and is 0 otherwise. We
conclude that $Q_G(\mathbf{1}_S)$ sums the weights of the edges on the cut
$S$ \cite{TheSurvey}:
\begin{align*}
    \cut{G}(S) = Q_G(\mathbf{1}_S)
\end{align*}
At this point we could redefine cut similarity by restricting $Q_G$ to $\{0,
1\}^n$. We get a stronger criterion by considering the entire domain.  The
graphs $(G, \tilde{G})$ are \textit{$\epsilon$-spectrally similar} if:
\begin{align*}
    (1 - \epsilon)Q_{\tilde{G}}(x) \leq Q_G(x) \leq (1 +
    \epsilon)Q_{\tilde{G}}(x) \quad \forall{x \in \Re^n}
\end{align*}

There is an intuitive way to see that cut similarity is not as strong as
spectral similarity. Consider a $k$-cycle and a $k$-path. We can make the
$k$-path a $k$-cycle by adding one edge, which we call $e'$. Most cuts on
the vertex set $\{1..k\}$ have identical weights on both graphs because they
don't touch $e'$, making the graphs fairly cut-similar. However, we know
that the $k$-cycle is 2-connected while the $k$-path isn't. We conclude that
the Fiedler values of the two graphs are dissimilar, which in turn implies
spectral dissimilarity \cite{Fiedler}. Apart from connectivity, spectrally
similar graphs are more likely to share a host of other structural
characteristics captured by Laplacian quadratic form.

Spectrally similar graphs also share algebraic properties. Consider the
problem of finding vertex labellings $(l, \tilde{l})$ for
$\epsilon$-spectrally similar graphs $(G, \tilde{G})$ with some of the
labels fixed a priori. We can pose this as a regression problem. First solve
for the unknowns in $l \in \Re^n$ by minimizing $Q_G(l)$, then use the
result to approximate $\tilde{l}$. Intuitively, we expect to find a good
approximation because $Q_G(x)$ and $Q_{\tilde{G}}(x)$ match very well over
$\Re^n$.

\subsection{Building spectral sparsifiers}

The spectral similarity criterion is very strong. It is not obvious that
every graph has a sparse, spectrally-similar cousin. We need some linear
algebra to find these spectral sparsifiers. The central object we are
interested in is the $n \times n$ graph \textit{Laplacian matrix}:
\begin{align*}
    L_G =
    \begin{cases}
        -w(u, v) \quad &\text{if } u \not= v \\
        \sum_{z} w(u, z) \quad &\text{if } u = v
    \end{cases}
\end{align*}
We can use $L_G$ to simplify the Laplacian quadratic form:
\begin{align*}
    Q_G(x) = x^TL_Gx
\end{align*}
To see why this is true, recall that the quadratic form of a matrix $A$ is
$x^TAx = \sum_{i,j} A_{i,j}x_ix_j$.  Every edge $(u, v)$ in $G$ contributes
$w(u, v)[x_u^2 - 2x_ux_v + x_v^2]$ to the sum $Q_G(x)$. If we add up the
following contributions, it becomes clear that our two definitions are
equivalent:
\begin{align*}
    i = u, j = u &:\quad w(u, v)x_u^2 \\
    i = v, j = v &:\quad w(u, v)x_v^2 \\
    i = u, j = v &:\quad -w(u, v)x_ux_v \\
    i = v, j = u &:\quad -w(u, v)x_ux_v
\end{align*}
We can order symmetric matrices in $\Re^{n \times n}$ by looking at their
quadratic forms:
\begin{align*}
    A \preceq B \Leftrightarrow x^TAx \leq x^TBx \quad \forall{x \in \Re^n}
\end{align*}
Now we can check if two graphs are $\epsilon$-spectrally similar by
comparing their Laplacians:
\begin{align*}
    (1 - \epsilon)L_{\tilde{G}} \preceq L_G \preceq (1 +
    \epsilon)L_{\tilde{G}}
\end{align*}
The `$\preceq$' relation would be more useful if there were a simple way to
compute it. By the Courant-Fischer theorem \cite{CourantFischer}, the $i$-th
eigenvalue of a matrix is given by:
\begin{align*}
    \lambda_i(A) = \max_{S: \dim(S) = i} \min_{x \in S} \frac{x^TAx}{x^Tx}
\end{align*}
Note that the quadratic form of $A$ appears in $\lambda_i(A)$. We conclude
that for $\epsilon$-spectrally similar graphs, the spectra of the graph
Laplacians are similar:
\begin{align*}
    (1 - \epsilon)\lambda_i(L_{\tilde{G}}) \leq \lambda_i(L_G) \leq (1 +
    \epsilon)\lambda_i(L_{\tilde{G}}) \quad \forall{i}
\end{align*}
This fact implies that edge weights are preserved between spectrally similar
graphs because \footnote{We have used the fact that the Laplacian is positive
semi-definite here, which is shown in Section 3.}:
\begin{align*}
    \Tr(L_G) = 2\sum_{e \in E} w(e) = \sum_i \lambda_i(L_G) \approx
    \Tr(L_{\tilde{G}}) = 2\sum_{e \in \tilde{E}} \tilde{w}(e)
\end{align*}
This demonstrates that edges in spectral sparsifiers literally `carry more
weight', and play a greater role in maintaining the structure of the graph.

We are interested in building spectral sparsifiers using edge-sampling s.t
$\tilde{G} \subset G$. In order to achieve spectral similarity, we would
like $L_{\tilde{G}}$ to be close to $L_G$ in expectation. We can enforce
$E[L_{\tilde{G}}] = L_G$ with the following process:
\begin{itemize}
    \item Assign each edge a selection probability $p_e$.
    \item Initialize all edges weights $\tilde{w}_e$ in $\tilde{G}$ to 0:
        when an edge $e$ is added to $\tilde{G}$, add $w_e/p_e$ to
        $\tilde{w}_e$.
    \item Sample edges from $G$ until $\tilde{G}$ is large enough.
\end{itemize}

If the edge selection probabilities are too high, $\tilde{G}$ becomes too
dense. If they are too low, the spectrum of $L_{\tilde{G}}$ may not be close
enough to the spectrum of $L_G$. The probabilities $p_e$ should reflect the
fact that some edges are more important than others. An early edge-sampling
scheme worked by splitting up the vertex set of $G$ into high-conductance
components, and by prioritizing the edges between these components
\cite{SpielmanTeng}. Roughly speaking, the conductance of a subset $S
\subset V$ is the number of edges on the cut boundary of $S$ divided by the
number of edges `contained' by the cut. This led to the following result: \\

\noindent
\textlcsc{Theorem 2 (Spielman-Teng)}: \textit{Let $\epsilon > 0$. $G = (V,
E)$ has an $\epsilon$-spectrally similar graph $\tilde{G} \subset G$ s.t
$\tilde{m} \in O(\epsilon^{-2}n\log^2 n)$. $\tilde{G}$ can be found in
$O(m\log^{O(1)}m)$ time \cite{SpielmanTeng} \cite{TheSurvey}.} \\

Theorem 2 implies the somewhat surprising fact that arbitrary graphs have
high-quality spectral sparsifiers. From here, our focus shifts to finding
improved time and space bounds for the sparsification algorithm. Section 3
covers a fast spectral sparsification algorithm in detail.

\section{Fast Sparsification with Sampling}

start by stating end result

A few facts about $L_G$:
\begin{enumerate}[1.]
    \item $L_G$ is symmetric.

        Proof: For the off-diagonal elements $u \not= v$, we have: $L_G(u,
        v) = L_G(v, u) = -w(u, v)$.

    \item $L_G$ 
\end{enumerate}

Cover the effective-resistances paper.

\section{Applications}

\bibliographystyle{unsrt}
\bibliography{ref}

\begin{enumerate}[1.]
    \item Achlioptas, D., Mcsherry, F. Fast computation of low-rank matrix
        approximations. 2 (2007), 9.

    \item Batson, J.D., Spielman, D.A., Srivastava, N.  Twice-Ramanujan
        sparsifiers. 6 (2012), 1704 –1721.

    \item Chandra, A.K., Raghavan, P., Ruzzo, W.L., Smolensky, R., Tiwari,
        P.  The electrical resistance of a graph captures its commute and
        cover times. In (1989), ACM, New York, NY, USA, 574–586.

    \item Spielman, D.A., Srivastava, N. Graph sparsification by effective
        resistances.  6 (2011), 1913–1926.
\end{enumerate}

\end{document}
