\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage[a4paper]{geometry}

\title{Spectral Graph Sparsification}
\author{\small{Vedant Kumar -- \texttt{vsk@berkeley.edu} -- CS270 Survey}}

\begin{document}
\maketitle

\newcommand \cut[1]{\text{cut}_{#1}}
\newcommand \textlcsc[1]{\textsc{\MakeLowercase{#1}}}

\section*{Abstract}

Graph sparsification algorithms find sparse approximations of graphs. In
this paper we survey major results in spectral sparsification, a special
case in which important structural and algebraic properties of graphs are
preserved. We explain a key result: that every graph has a spectral
sparsifier with $O(|V|\log|V|)$ edges w.h.p, and that such sparsifiers can
be found in $\tilde{O}(|E|)$ time. We finish by discussing some applications
of spectral sparsification, such as computing electrical flows and finding
min-cuts in nearly-linear time.

\section{Introduction}

Sparse graphs have a similar number of edges and vertices, up to a polylog
factor ($|E| \in \tilde{O}(|V|)$). In contrast, dense graphs (such as the
complete graph) permit a quadratic number of edges ($|E| \in O(|V|^2)$). If
the number of vertices is fixed, many graph algorithms run significantly
faster and with lower storage requirements on sparse inputs.  Moreover, each
edge in a sparse graph tends to contribute `more' to the overall shape and
structure of the graph.  Working with sparse graphs can save time, save
space, and provide insight into the nature of a graph: this makes graph
sparsification interesting.

In order to assess the quality of a sparsification algorithm, we need a way
to measure similarity between a graph and its sparse approximation. Section
2 discusses two different measures of graph similarity --
\textit{cut similarity} and \textit{spectral similarity} -- both of which
led to the discovery of important sparsification algorithms. Section 3
contains a detailed exposition of a fast spectral sparsification algorithm
based on an edge-sampling scheme.  Section 4 mentions some applications of
spectral sparsification, and Section 5 reprises some of the key ideas
presented in this survey.

\section{Graph Similarity}

In this section we explain how \textit{cut similarity} naturally leads to
\textit{spectral similarity}, a strictly stronger criterion for measuring
the quality of a sparsifier. 

\subsection{Conventions}

The only graphs we will consider are undirected, weighted, and connected. A
graph can be described by a tuple $G = (V, E, w)$, where $w : V^2
\rightarrow \Re$ maps edges to their weights. All edges must have positive
weights, so $w(u, v) = 0 \Leftrightarrow (u, v) \not\in E$, allowing us to
write $G = (V, w)$ w.l.o.g. For notational simplicity we define $n = |V|$
and $m = |E|$. When comparing two graphs, we always assume that they share
the same vertex set.

\subsection{Cut similarity}

Two graphs are \textit{cut similar} if all of their cuts have approximately
similar weights. To make this concrete, consider two graphs $G = (V, w)$ and
$\tilde{G} = (V, \tilde{w})$. A \textit{cut} is a subset of vertices $S
\subset V$.  The weight of a cut is given by the sum of the weights of edges
on the cut:
\begin{align*}
    \cut{G}(S) = \sum_{u \in S, v \in V - S} w(u, v)
\end{align*}
$G$ and $\tilde{G}$ are $\epsilon$-cut similar if for all possible cuts, we
have:
\begin{align*}
    (1 - \epsilon)\cut{\tilde{G}}(S) \leq \cut{G}(S) \leq (1 +
    \epsilon)\cut{\tilde{G}}(S) \quad \forall{S \subset V}
\end{align*}
An important early result states that every graph has a good cut similar
approximation with $\tilde{O}(n)$ edges, and that this approximation can be
found quickly: \\

\noindent
\textlcsc{Theorem 1 (Bencz\'{u}r-Karger)}: \textit{Let $\epsilon > 0$. $G =
(V, E)$ has an $\epsilon$-cut similar graph $\tilde{G} \subset G$ s.t
$\tilde{m} \in O(\epsilon^{-2}n\log n)$. $\tilde{G}$ can be found in
$O(m\log^3n + \epsilon^{-2}m\log n)$ time \cite{BenczurKarger}.} \\

This version of Theorem 1 is taken from a later survey \cite{TheSurvey}. The
original paper claims that an even better $O(m\log^2 n)$ time bound is
possible using the union-find data structure. Regardless, this result sets
the stage for future work by showing that high-quality cut-similar
sparsifiers exist for \textit{every} graph. 

\subsection{Spectral similarity}

The \textit{Laplacian quadratic form} of a graph $G = (V, E, w)$ is given by
the map $Q_G : \Re^n \rightarrow \Re$:
\begin{align*}
    Q_G(x) = \sum_{(u, v) \in E} w(u, v)(x(u) - x(v))^2
\end{align*}
Consider a cut $S \subset V$: the $i$-th component of its
\textit{characteristic vector} $\mathbf{1}_S$ is 1 if $v_i \in S$, and is 0
otherwise. The expression $(\mathbf{1}_S(u) - \mathbf{1}_S(v))^2$ is 1 iff
$(u, v)$ is an edge on the cut boundary of $S$, and is 0 otherwise. We
conclude that $Q_G(\mathbf{1}_S)$ sums the weights of the edges on the cut
$S$ \cite{TheSurvey}:
\begin{align*}
    \cut{G}(S) = Q_G(\mathbf{1}_S)
\end{align*}
At this point we could redefine cut similarity by restricting $Q_G$ to $\{0,
1\}^n$. We get a stronger criterion by considering the entire domain.  The
graphs $(G, \tilde{G})$ are \textit{$\epsilon$-spectrally similar} if:
\begin{align*}
    (1 - \epsilon)Q_{\tilde{G}}(x) \leq Q_G(x) \leq (1 +
    \epsilon)Q_{\tilde{G}}(x) \quad \forall{x \in \Re^n}
\end{align*}

There is an intuitive way to see that cut similarity is not as strong as
spectral similarity. Consider a $k$-cycle and a $k$-path. We can make the
$k$-path a $k$-cycle by adding one edge, which we call $e'$. Most cuts on
the vertex set $\{1..k\}$ have identical weights on both graphs because they
don't touch $e'$, making the graphs fairly cut-similar. However, we know
that the $k$-cycle is 2-connected while the $k$-path isn't. We conclude that
the Fiedler values of the two graphs are dissimilar, which in turn implies
spectral dissimilarity. Apart from connectivity, spectrally similar graphs
are more likely to share a host of other structural characteristics captured
by Laplacian quadratic form.

Spectrally similar graphs also share algebraic properties. Consider the
problem of finding vertex labellings $(l, \tilde{l})$ for
$\epsilon$-spectrally similar graphs $(G, \tilde{G})$ with some of the
labels fixed a priori. We can pose this as a regression problem. First solve
for the unknowns in $l \in \Re^n$ by minimizing $Q_G(l)$, then use the
result to approximate $\tilde{l}$. Intuitively, we expect to find a good
approximation because $Q_G(x)$ and $Q_{\tilde{G}}(x)$ match very well over
$\Re^n$.

\subsection{Building spectral sparsifiers}

Given the strength of the spectral similarity criterion it is not obvious
that every graph has a sparse, spectrally-similar cousin. We need some
linear algebra to find these spectral sparsifiers. The central object we are
interested in is the $n \times n$ \textit{Laplacian matrix} of a graph:
\begin{align*}
    L_G =
    \begin{cases}
        -w(u, v) \qquad &\text{if } u \not= v \\
        \sum_{z \in V} w(u, z) \qquad &\text{if } u = v
    \end{cases}
\end{align*}
We can use $L_G$ to simplify the Laplacian quadratic form: $Q_G(x) =
x^TL_Gx$. To see why this is true, recall that the quadratic form of a
matrix $A$ is given by $x^TAx = \sum_{i,j} A_{i,j}x_ix_j$.  Every edge $(u,
v)$ in $G$ contributes $w(u, v)[x_u^2 - 2x_ux_v + x_v^2]$ to the sum
$Q_G(x)$. If we add up the following contributions, it becomes clear that
our two definitions are equivalent:
\begin{align*}
    i = u, j = u &:\quad w(u, v)x_u^2 \\
    i = v, j = v &:\quad w(u, v)x_v^2 \\
    i = u, j = v &:\quad -w(u, v)x_ux_v \\
    i = v, j = u &:\quad -w(u, v)x_ux_v
\end{align*}
We need a way to 
Now we define the $\preceq$ relation to order matrices:
\begin{align*}
    x^TAx \preceq x^TBx \qquad \forall{x \in \Re^n}
\end{align*}


Describe Spielman-Teng, go up to thm 4.

\section{Fast Sparsification with Sampling}

A few facts about $L_G$:
\begin{enumerate}[1.]
    \item $L_G$ is symmetric.

        Proof: For the off-diagonal elements $u \not= v$, we have: $L_G(u,
        v) = L_G(v, u) = -w(u, v)$.

    \item $L_G$ 
\end{enumerate}

Cover the effective-resistances paper.

\section{Applications}

\section{Overview}

\cite{CourantFischer}

\bibliographystyle{unsrt}
\bibliography{ref}

\begin{enumerate}[1.]
    \item Achlioptas, D., Mcsherry, F. Fast computation of low-rank matrix
        approximations. 2 (2007), 9.

    \item Batson, J.D., Spielman, D.A., Srivastava, N.  Twice-Ramanujan
        sparsifiers. 6 (2012), 1704 –1721.

    \item Batson, J.D., Spielman, D.A., Srivastava, N, Teng, S.  Spectral
        Sparsification of Graphs: Theory and Applications. 8 (2013),
        Communications of the ACM.

    \item Bencz\'{u}r, A.A., Karger, D.R.  Approximating s-t minimum cuts in
        $O(n^2)$ time. In (1996), 47–55.

    \item Chandra, A.K., Raghavan, P., Ruzzo, W.L., Smolensky, R., Tiwari,
        P.  The electrical resistance of a graph captures its commute and
        cover times. In (1989), ACM, New York, NY, USA, 574–586.

    \item Cheeger, J. A lower bound for smallest eigenvalue of Laplacian. In
        (1970), Princeton University Press, 195–199.

    \item Spielman, D.A., Srivastava, N. Graph sparsification by effective
        resistances.  6 (2011), 1913–1926.
\end{enumerate}

\end{document}
