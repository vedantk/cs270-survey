\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage[a4paper]{geometry}

\title{Spectral Graph Sparsification}
\author{\small{Vedant Kumar -- \texttt{vsk@berkeley.edu} -- CS270 Survey}}

\begin{document}
\maketitle

\newcommand \cut[1]{\text{cut}_{#1}}
\newcommand \textlcsc[1]{\textsc{\MakeLowercase{#1}}}

\section*{Abstract}

Graph sparsification algorithms find sparse approximations of graphs. In
this paper we survey major results in spectral sparsification, a special
case in which important structural and algebraic properties of graphs are
preserved. We explain a key result: that every graph has a spectral
sparsifier with $O(|V|\log|V|)$ edges w.h.p, and that such a sparsifier can
be found in $\tilde{O}(|E|)$ time. We finish by discussing some applications
of spectral sparsification, such as computing electrical flows and finding
min-cuts in nearly-linear time.

\section{Introduction}

Sparse graphs have a similar number of edges and vertices, up to a polylog
factor ($|E| \in \tilde{O}(|V|)$). In contrast, dense graphs (such as the
complete graph) permit a quadratic number of edges ($|E| \in O(|V|^2)$). If
the number of vertices is fixed, many graph algorithms run significantly
faster and with lower storage requirements on sparse inputs.  Moreover, each
edge in a sparse graph tends to contribute `more' to the overall shape and
structure of the graph.  Working with sparse graphs can save time, save
space, and provide insight into the nature of a graph: this makes graph
sparsification interesting.

In order to assess the quality of a sparsification algorithm, we need a way
to measure similarity between a graph and its sparse approximation. Section
2 discusses two different measures of graph similarity --
\textit{cut similarity} and \textit{spectral similarity} -- both of which
led to the discovery of important sparsification algorithms. Section 3
contains a detailed exposition of a fast spectral sparsification algorithm
based on an edge-sampling scheme.  Section 4 mentions some applications of
spectral sparsification, and Section 5 reprises some of the key ideas
presented in this survey.

\section{Graph Similarity}

In this section we explain how \textit{cut similarity} naturally leads to
\textit{spectral similarity}, a strictly stronger criterion. These
similarity measures ground all further discussion of `good' sparsifiers.

\subsection{Conventions}

The only graphs we will consider are undirected, weighted, and connected. A
graph can be described by a tuple $G = (V, E, w)$, where $w : V^2
\rightarrow \Re$ maps edges to their weights. All edges must have positive
weights, so $w(u, v) \leq 0 \Rightarrow (u, v) \not\in E$, allowing us to
write $G = (V, w)$ w.l.o.g. For notational simplicity we define $n = |V|$
and $m = |E|$. When comparing two graphs, we always assume that they share
the same vertex set.

\subsection{Cut similarity}

Two graphs are \textit{cut similar} if all of their cuts have approximately
similar weights. To make this concrete, consider two graphs $G = (V, w)$ and
$H = (V, \tilde{w})$. A \textit{cut} is a subset of vertices $S \subset V$.
The weight of a cut is given by the sum of the weights of edges on the cut:
\begin{align*}
    \cut{G}(S) = \sum_{u \in S, v \in V - S} w(u, v)
\end{align*}
$G$ and $H$ are $\epsilon$-cut similar if for all possible cuts, we have:
\begin{align*}
    (1 - \epsilon)\cut{H}(S) \leq \cut{G}(S) \leq (1 + \epsilon)\cut{H}(S)
    \quad \forall{S \subset V}
\end{align*}
An important early result states that every graph has a good cut similar
approximation with $\tilde{O}(n)$ edges, and that this approximation can be
found quickly: \\

\noindent
\textlcsc{Theorem 1 (Bencz\'{u}r-Karger)}: \textit{Let $\epsilon > 0$. $G =
(V, E)$ has an $\epsilon$-cut similar graph $\tilde{G} \subset G$ s.t
$\tilde{m} \in O(\epsilon^{-2}n\log n)$. $\tilde{G}$ can be found in
$O(m\log^3n + \epsilon^{-2}m\log n)$ time.} \footnote{cite benczur-karger
paper} \\

This version of Theorem 1 is taken from a later survey \footnote{cite
survey}. The original paper claims that an even better $O(m\log^2 n)$ time
bound is possible using the union-find data structure. Regardless, this
result sets the stage for future work by showing that high-quality
cut-similar sparsifiers exist for \textit{every} graph. 

\subsection{Spectral similarity}

The \textit{Laplacian quadratic form} of a graph $G = (V, E, w)$ is given by
the map $Q_G : \Re^n \rightarrow \Re$:
\begin{align*}
    Q_G(x) = \sum_{(u, v) \in E} w(u, v)(x(u) - x(v))^2
\end{align*}
Consider a cut $S \subset V$: the $i$-th component of its
\textit{characteristic vector} $\mathbf{1}_S$ is 1 if $v_i \in S$, and is 0
otherwise. The expression $(\mathbf{1}_S(u) - \mathbf{1}_S(v))^2$ is 1 iff
$(u, v)$ is an edge on the cut boundary of $S$, and is 0 otherwise. We
conclude that $Q_G(\mathbf{1}_S)$ sums the weights of the edges on the cut
$S$ \footnote{cite survey}:
\begin{align*}
    \cut{G}(S) = Q_G(\mathbf{1}_S)
\end{align*}
At this point we could redefine cut similarity by restricting $Q_G$ to $\{0,
1\}^n$. We get a much stronger criterion by considering the entire domain.
The graphs $G, H$ are \textit{$\epsilon$-spectrally similar} if:
\begin{align*}
    (1 - \epsilon)Q_H(x) \leq Q_G(x) \leq (1 + \epsilon)Q_H(x) \quad
    \forall{x \in \Re^n}
\end{align*}

There is an intuitive way to see that cut similarity is not as strong as
spectral similarity. Consider a $k$-cycle and a $k$-path. We can make the
$k$-path a $k$-cycle by adding one edge, which we call $e'$. Most cuts on
the vertex set $\{1..k\}$ have identical weights on both graphs because they
don't touch $e'$, making the graphs fairly cut-similar. However, we know
that the $k$-cycle is 2-connected while the $k$-path isn't. We conclude that
the Fiedler vectors of the two graphs are dissimilar, which in turn implies
spectral dissimilarity.

Describe Spielman-Teng, go up to thm 4.

\section{Fast Sparsification with Sampling}

Cover the effective-resistances paper.

\section{Applications}

\section{Overview}

\section*{References}

\begin{enumerate}[1.]
    \item Achlioptas, D., Mcsherry, F. Fast computation of low-rank matrix
        approximations. 2 (2007), 9.

    \item Batson, J.D., Spielman, D.A., Srivastava, N.  Twice-Ramanujan
        sparsifiers. 6 (2012), 1704 –1721.

    \item Batson, J.D., Spielman, D.A., Srivastava, N, Teng, S.  Spectral
        Sparsification of Graphs: Theory and Applications. 8 (2013),
        Communications of the ACM.

    \item Bencz\'{u}r, A.A., Karger, D.R.  Approximating s-t minimum cuts in
        $O(n^2)$ time. In (1996), 47–55.

    \item Chandra, A.K., Raghavan, P., Ruzzo, W.L., Smolensky, R., Tiwari,
        P.  The electrical resistance of a graph captures its commute and
        cover times. In (1989), ACM, New York, NY, USA, 574–586.

    \item Cheeger, J. A lower bound for smallest eigenvalue of Laplacian. In
        (1970), Princeton University Press, 195–199.

    \item Spielman, D.A., Srivastava, N. Graph sparsification by effective
        resistances.  6 (2011), 1913–1926.
\end{enumerate}

\end{document}
