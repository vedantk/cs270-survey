\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage[a4paper]{geometry}

\title{Spectral Graph Sparsification}
\author{\small{Vedant Kumar -- \texttt{vsk@berkeley.edu} -- CS270 Survey}}

\begin{document}
\maketitle

\newcommand \R{\mathbb{R}}
\newcommand \one{\mathbf{1}}
\newcommand \zero{\mathbf{0}}
\newcommand \Tr{\text{Tr}}
\newcommand \im{\text{im}}
\newcommand \Span{\text{span}}
\newcommand \cut[1]{\text{cut}_{#1}}
\newcommand \textlcsc[1]{\textsc{\MakeLowercase{#1}}}

\section*{Abstract}

Graph sparsification algorithms find sparse approximations of graphs. In
this paper we survey major results in spectral sparsification, a special
case in which important structural and algebraic properties of graphs are
preserved. We explain a key result: that every graph has a spectral
sparsifier with $O(|V|\log|V|)$ edges w.h.p, and that such sparsifiers can
be found in $\tilde{O}(|E|)$ time. We finish by discussing an application of
spectral sparsification to the problem of computing electrical flows in a
circuit in nearly-linear time.

\section{Introduction}

Sparse graphs have roughly the same number of edges and vertices, up to a
polylog factor ($|E| \in \tilde{O}(|V|)$). In contrast, dense graphs (such
as the complete graph) permit a quadratic number of edges ($|E| \in
O(|V|^2)$). If the number of vertices is fixed, many graph algorithms run
significantly faster and with lower storage requirements on sparse inputs.
Moreover, each edge in a sparse graph tends to contribute `more' to the
overall shape and structure of the graph.  Working with sparse graphs can
save time, save space, and provide insight into the nature of a graph: this
makes graph sparsification interesting.

In order to evaluate sparsification algorithms, we need a way to measure
`similarity' between a graph and its sparse approximation. Section 2
discusses two different measures of graph similarity -- \textit{cut
similarity} and \textit{spectral similarity} -- both of which led to the
discovery of important sparsification algorithms. Section 3 contains a
detailed exposition of a fast spectral sparsification algorithm based on an
edge-sampling scheme. Section 4 discusses a circuit-related application.

\section{Graph Similarity}

In this section we explain how \textit{cut similarity} naturally leads to
\textit{spectral similarity}, a strictly stronger measure of sparsifier
quality. We also mention influential results in sparsification.

\subsection{Conventions}

The only graphs we consider are undirected, weighted, and connected. A graph
can be described by a tuple $G = (V, E, w)$, where $w : V^2 \rightarrow \R$
maps edges to weights. All edges must have positive weights, so $w(u, v) = 0
\Leftrightarrow (u, v) \not\in E$, allowing us to write $G = (V, w)$
w.l.o.g. For notational simplicity we define $n = |V|$ and $m = |E|$. When
comparing two graphs, we always assume that they share the same vertex set.
Finally, we use the notation $A_k$ to refer to the $k$-th column vector of
some matrix $A$. 

\subsection{Cut similarity}

In this section we describe cut similarity and cite an early result in the
field. Two graphs are \textit{cut similar} if all of their cuts have
approximately similar weights. To make this concrete, consider two graphs $G
= (V, w)$ and $\tilde{G} = (V, \tilde{w})$. A \textit{cut} is a subset of
vertices $S \subset V$.  The weight of a cut is given by the sum of the
weights of edges on the cut:
\begin{align*}
    \cut{G}(S) = \sum_{u \in S, v \in V - S} w(u, v)
\end{align*}
$G$ and $\tilde{G}$ are $\epsilon$-cut similar if for all possible cuts, we
have:
\begin{align*}
    (1 - \epsilon)\cut{\tilde{G}}(S) \leq \cut{G}(S) \leq (1 +
    \epsilon)\cut{\tilde{G}}(S) \quad \forall{S \subset V}
\end{align*}
An important early result states that every graph has a good cut similar
approximation with $\tilde{O}(n)$ edges, and that this approximation can be
found quickly: \\

\noindent
\textlcsc{Theorem 1 (Bencz\'{u}r-Karger)}: \textit{Let $\epsilon > 0$. $G =
(V, E)$ has an $\epsilon$-cut similar subgraph $\tilde{G}$ s.t $\tilde{m}
\in O(\epsilon^{-2}n\log n)$. $\tilde{G}$ can be found in $O(m\log^3n +
\epsilon^{-2}m\log n)$ time \cite{BenczurKarger}.} \\

This version of Theorem 1 is taken from a later survey \cite{TheSurvey}. The
original paper claims that an even better $O(m\log^2 n)$ time bound is
possible using the union-find data structure. Regardless, this result sets
the stage for future work by showing that high-quality cut-similar
sparsifiers exist for \textit{every} graph. 

\subsection{Spectral similarity}

In this section we motivate and define spectral similarity.

The \textit{Laplacian quadratic form} of a graph $G = (V, E, w)$ is given by
the map $Q_G : \R^n \rightarrow \R$:
\begin{align*}
    Q_G(x) = \sum_{(u, v) \in E} w(u, v)(x(u) - x(v))^2
\end{align*}
Consider a cut $S \subset V$: the $i$-th component of its
\textit{characteristic vector} $\one_S$ is 1 if $v_i \in S$, and is 0
otherwise. The expression $(\one_S(u) - \one_S(v))^2$ is 1 iff $(u, v)$ is
an edge on the cut boundary of $S$, and is 0 otherwise. We conclude that
$Q_G(\one_S)$ sums the weights of the edges on the cut $S$:
\begin{align*}
    \cut{G}(S) = Q_G(\one_S)
\end{align*}
We get cut similarity by restricting $Q_G$ to $\{0, 1\}^n$. The graphs $(G,
\tilde{G})$ are \textit{$\epsilon$-spectrally similar} if:
\begin{align*}
    (1 - \epsilon)Q_{\tilde{G}}(x) \leq Q_G(x) \leq (1 +
    \epsilon)Q_{\tilde{G}}(x) \quad \forall{x \in \R^n}
\end{align*}

There is an intuitive way to see that cut similarity is not as strong as
spectral similarity. Consider a $k$-cycle and a $k$-path. We can make the
$k$-path a $k$-cycle by adding an edge $e'$. Most cuts on $\{1..k\}$ have
identical weights on both graphs because they don't touch $e'$, so the
graphs are fairly cut-similar. However, we know that the $k$-cycle is
2-connected while the $k$-path isn't. We conclude that the graphs have
dissimilar $\lambda_2$'s, the bipartitedness eigenvalue, implying spectral
dissimilarity \cite{Fiedler}. Apart from connectivity, spectrally similar
graphs are more likely to share several other structural characteristics
captured by the Laplacian quadratic form.

Spectrally similar graphs also share algebraic properties. Consider the
problem of finding vertex labellings $(l, \tilde{l})$ for
$\epsilon$-spectrally similar graphs $(G, \tilde{G})$ with some of the
labels fixed a priori. We can pose this as a regression problem. First solve
for the unknowns in $l \in \R^n$ by minimizing $Q_G(l)$, then use the
result to approximate a full labelling $\tilde{l}$. Intuitively, we expect
to find a good approximation because $Q_G(x)$ and $Q_{\tilde{G}}(x)$ match
very well over $\R^n$ \cite{TheSurvey}.

\subsection{The Laplacian quadratic form}

In this section we explain some important properties of the Laplacian
quadratic form. We rely on these properties heavily throughout the rest of
this survey. The central object we are interested in is the $n \times n$
graph \textit{Laplacian matrix}:
\begin{align*}
    L_G &= D - A \\
        &= 
    \begin{cases}
        -w(u, v) \quad &\text{if } u \not= v \\
        \sum_{z} w(u, z) \quad &\text{if } u = v
    \end{cases}
\end{align*}
Here, $D$ is a diagonal matrix of vertex degrees and $A$ is the adjacency
matrix.  We can use $L_G$ to simplify the Laplacian quadratic form:
\begin{align*}
    Q_G(x) = x^TL_Gx
\end{align*}
To see why this is true, recall that the quadratic form of a matrix $A$ is
$x^TAx = \sum_{i,j} A_{i,j}x_ix_j$.  Every edge $(u, v)$ in $G$ contributes
$w(u, v)[x_u^2 - 2x_ux_v + x_v^2]$ to the sum $Q_G(x)$. If we add up the
following contributions, it becomes clear that our two definitions are
equivalent:
\begin{align*}
    i = u, j = u &:\quad w(u, v)x_u^2 \\
    i = v, j = v &:\quad w(u, v)x_v^2 \\
    i = u, j = v &:\quad -w(u, v)x_ux_v \\
    i = v, j = u &:\quad -w(u, v)x_ux_v
\end{align*}
We can order symmetric PSD matrices by looking at their quadratic forms:
\begin{align*}
    A \preceq B \Leftrightarrow x^TAx \leq x^TBx \quad \forall{x \in \R^n}
\end{align*}
Now we can check if two graphs are $\epsilon$-spectrally similar by
comparing their Laplacians \footnote{We have used the fact that the
Laplacian is positive semi-definite here, which is shown in Section 3.}:
\begin{align*}
    (1 - \epsilon)L_{\tilde{G}} \preceq L_G \preceq (1 +
    \epsilon)L_{\tilde{G}}
\end{align*}
The `$\preceq$' relation would be more useful if there were a simple way to
compute it. By the Courant-Fischer theorem \cite{CourantFischer}, the $i$-th
eigenvalue of a matrix is given by:
\begin{align*}
    \lambda_i(A) = \max_{S: \dim(S) = i} \min_{x \in S} \frac{x^TAx}{x^Tx}
\end{align*}
Note that the quadratic form of $A$ appears in $\lambda_i(A)$. We conclude
that for $\epsilon$-spectrally similar graphs, the eigenvalues of the graph
Laplacians are similar:
\begin{align*}
    (1 - \epsilon)\lambda_i(L_{\tilde{G}}) \leq \lambda_i(L_G) \leq (1 +
    \epsilon)\lambda_i(L_{\tilde{G}}) \quad \forall{i}
\end{align*}
This fact can be used to show that the total edge weight of a graph is
roughly preserved by an $\epsilon$-spectral approximation:
\begin{align*}
    \Tr(L_G) = 2\sum_{e \in E} w(e) = \sum_i \lambda_i(L_G) \approx
    \Tr(L_{\tilde{G}}) = 2\sum_{e \in \tilde{E}} \tilde{w}(e)
\end{align*}
Edges in spectral sparsifiers literally `carry more weight' and play a
greater role in maintaining the structure of the graph.

\subsection{Building spectral sparsifiers}

In this section, we discuss a result which shows that every graph has a
sparse, spectrally-similar cousin. We are interested in building spectral
sparsifiers using edge-sampling s.t $\tilde{G} \subset G$. In order to
achieve spectral similarity, we would like $L_{\tilde{G}}$ to be close to
$L_G$ in expectation. We can enforce $E[L_{\tilde{G}}] = L_G$ with the
following process:
\begin{itemize}
    \item Assign each edge a selection probability $p_e$ (most of our effort
        is expended here).
    \item Initialize all edge weights $\tilde{w}_e$ in $\tilde{G}$ to 0:
        when an edge $e$ is added to $\tilde{G}$, add $w_e/p_e$ to
        $\tilde{w}_e$.
    \item Sample edges from $G$ until $\tilde{G}$ is large enough.
\end{itemize}

If the edge selection probabilities are too high, $\tilde{G}$ becomes too
dense. If they are too low, the spectrum of $L_{\tilde{G}}$ may not be close
enough to the spectrum of $L_G$. The probabilities $p_e$ should reflect the
fact that some edges are more important than others.

Informally, the \textit{conductance} of a subset $S \subset V$ is the number
of edges on the cut boundary of $S$ divided by the number of edges
`contained' by the cut.  An early edge-sampling scheme worked by splitting
up the vertex set of $G$ into high-conductance components, and by
prioritizing the edges between these components. This led to the following
result: \\

\noindent
\textlcsc{Theorem 2 (Spielman-Teng)}: \textit{Let $\epsilon > 0$. W.h.p $G =
(V, E)$ has an $\epsilon$-spectrally similar subgraph $\tilde{G}$ s.t
$\tilde{m} \in O(\epsilon^{-2}n\log^2 n)$. $\tilde{G}$ can be found in
$O(m\log^{O(1)}m)$ time \cite{SpielmanTeng} \cite{TheSurvey}.} \\

Theorem 2 implies the somewhat surprising fact that arbitrary graphs have
high-quality spectral sparsifiers. From here, our focus shifts to finding
improved time and space bounds for the sparsification algorithm.

\section{Fast Spectral Sparsification}

The goal of this section is to examine the following fast spectral
sparsification algorithm in detail: \\

\noindent
\textlcsc{Theorem 3 (Spielman-Srivastava)}: \textit{Let $\epsilon > 0$.
W.h.p $G = (V, E)$ has an $\epsilon$-spectrally similar subgraph $\tilde{G}$
s.t $\tilde{m} \in O(\epsilon^{-2}n\log n)$. $\tilde{G}$ can be found in
$\tilde{O}(m)$ time \cite{SpielmanSrivastava} \cite{TheSurvey}.} \\

The Spielman-Srivastava algorithm has two stages: (1) compute edge selection
probabilities, and (2) sample edges. The first step requires solving $\log
m$ Laplacian systems, for which the best known time bound is $O(m \log m
\log n \log\log n)$ \cite{FastLaplacianSolver} \cite{TheSurvey}. The second
step can be completed in $O(\epsilon^{-2}n\log n)$ time, for a total runtime
of $\tilde{O}(m)$.

\subsection{Preliminaries}

We need some basic facts and definitions to explain Theorem 3. Let each edge
in $G$ have some fixed (possibly arbitrary) orientation. The $(m \times n)$
\textit{signed edge-vertex incidence matrix} is:
\begin{align*}
    B(e, v) =
    \begin{cases}
        1 \quad &\text{if } e = (v, u) \\
        -1 \quad &\text{if } e = (u, v) \\
        0 \quad &\text{otherwise}
    \end{cases}
\end{align*}
The diagonal $(m \times m)$ \textit{edge weight matrix} is: $W(e, e) =
w(e)$. The Laplacian matrix $L$ can be written in terms of edge weights
and edge-vertex incidences:
\begin{align*}
    L = B^TWB
\end{align*}
To see why this is true, consider the 3-cycle (a triangle) where all edge
weights are 1. We have:
\begin{align*}
    B^T =
    \begin{pmatrix}
        1 & 0 & -1 \\
        -1 & 1 & 0 \\
        0 & -1 & 1
    \end{pmatrix} \quad
    B =
    \begin{pmatrix}
        1 & -1 & 0 \\
        0 & 1 & -1 \\
        -1 & 0 & 1
    \end{pmatrix} \quad
    L = B^TB =
    \begin{pmatrix}
        2 & -1 & -1 \\
        -1 & 2 & -1 \\
        -1 & -1 & 2
    \end{pmatrix}
\end{align*}
The $i$-th diagonal element of $B^TB$ is given by $B^T_iB_i$, which counts
the number of edges in the neighborhood of vertex $i$ (i.e $\sum_z w(i,
z)$). For other entries $B^T_jB_i$ ($i \not= j$), all non-zero terms in the
inner product must be negative because they correspond to one of $(i, j)$ or
$(j, i)$ (i.e $-w(i, j)$). The result is exactly the graph Laplacian.

We now verify that $L$ is positive semi-definite (PSD):
\begin{align*}
    x^TLx = x^TB^TWBx = ||W^{1/2}Bx||^2_2 \geq 0 \quad \forall{x
    \in \R^n}
\end{align*}
The null space of $L$ is the subspace spanned by $\one$. I.e, $\ker(L) =
\ker(W^{1/2}B) = \Span(\one)$:
\begin{align*}
    x^TLx = 0 &\Leftrightarrow ||W^{1/2}Bx||^2_2 = 0 \\
                &\Leftrightarrow x(u) - x(v) = 0 \quad \text{for all edges }
    (u, v) \in E \text{ since } Q_G(x) = 0 \\
    &\Leftrightarrow x \text{ is constant}
\end{align*}
$L$ can be diagonalized using the singular value decomposition. Let
$\lambda_{1..n-1}$ be the non-zero eigenvalues and $u_{1..n-1}$ the
eigenvectors:
\begin{align*}
    L = \sum_{i=1}^{n-1} \lambda_iu_iu^T_i = U\Sigma V^*
\end{align*}
The pseudo-inverse is given by:
\begin{align*}
    L^+ = \sum_{i=1}^{n-1} \frac{1}{\lambda_i}u_iu^T_i = V\Sigma^+ U^*
\end{align*}
When we are restricted to $\im(L) = \Span(\one)^{\bot}$, $LL^+$ is the
identity:
\begin{align*}
    LL^+ &= U\Sigma V^*V\Sigma^+ U^*
             = U\Sigma I \Sigma^+ U^*
             = U I U^*
             = I \quad \text{if } x \not\in \ker(L) \\
    L^+L &= V\Sigma^+ U^*U\Sigma V^*
             = V\Sigma^+ I \Sigma V^*
             = V I V^*
             = I \quad \text{if } x \not\in \ker(L)
\end{align*}
We need the pseudo-inverse because $L$ is not full-rank, and $\zero$ is not
invertible.

\subsection{The Projection matrix}

In this section we define the Projection matrix $\Pi$ and prove some facts
about it. Later, we show that $\Pi$ can be used to bound the spectral
approximation error.  To define $\Pi$, we need the $(m \times m)$ symmetric
\textit{resistance matrix} $R = BL^+B^T$, with diagonal entries ($R(e,
e) = R_e$) \footnote{We explain the significance of $R$ in Section 4.}.  The
$(m \times m)$ \textit{projection matrix} is $\Pi = W^{1/2}RW^{1/2}$. We
prove some properties of the projection matrix:
\begin{enumerate}[1.]
    \item $\Pi$ is a projection matrix.
        
        Proof: The fact that $\Pi^2 = \Pi$ can be checked mechanically. In
        the last step, we have $\Pi^2 = W^{1/2}BL^+LL^+B^TW^{1/2} =
        W^{1/2}BL^+B^TW^{1/2} = \Pi$, and we need the fact that $LL^+$ is
        the identity on $\im(L^+)$.

    \item The image of $\Pi$ is equal to the image of $W^{1/2}B$.
        
        Proof: First note that $\im(\Pi) \subseteq \im(W^{1/2}B)$.
        \begin{align*}
            \im(\Pi) = \im(W^{1/2}BL^+B^TW^{1/2}) \subseteq \im(W^{1/2}B)
        \end{align*}
        Any vector $y \in \im(W^{1/2}B)$ must satisfy $W^{1/2}Bx$ for some
        $x \not\in \ker(L)$, which means:
        \begin{align*}
            \Pi y &= W^{1/2}BL^+B^TW^{1/2}W^{1/2}Bx \\
                  &= W^{1/2}BL^+Lx \\
                  &= W^{1/2}Bx \quad \text{since } x \not\in \ker(L) \\
                  &= y \in \im(\Pi)
        \end{align*}
        So $\im(\Pi) = \im(W^{1/2}B)$.

    \item The eigenvalues of $\Pi$ are 1 ($n-1$ copies) and 0 ($m-n+1$
        copies).

        Proof: By rank-nullity on $L$, $\dim(\ker(L)) + \dim(\im(L)) = 1 +
        (n-1) = n$. From this we have $\dim(\im(\Pi)) = n-1$. Because $\Pi$
        is a projection matrix all of its eigenvalues are either 1 or 0:
        since the image of $\Pi$ has dimension $n-1$ it must have $n-1$
        non-zero (`1') eigenvalues. The remaining $m-n+1$ eigenvalues must
        be zeros.

    \item The diagonal elements of $\Pi$ are: $\Pi(e, e) = \sqrt{W(e,
        e)}R_e\sqrt{W(e, e)} = w(e)R_e$. We know $\Pi$ is symmetric by
        symmetry of $W$ and $R$, so we get $\Pi(e, e) = \Pi^2(e, e) =
        \Pi_e^T\Pi_e = ||\Pi_e||^2_2$.
\end{enumerate}

\subsection{Linking the Laplacian and Projection matrices}

Our goal is to find a sparsifier $\tilde{G}$ s.t the quadratic forms of $L$
and $\tilde{L}$ are close. In this section, we show that sparsifiers which
keep the quadratic forms of $\Pi$ and $\tilde{\Pi}$ close achieve this. The
first step towards showing this is finding a symbolic expression for
$\tilde{L}$. If we sample $q$ edges to build $\tilde{G}$, we have a random
$(m \times m)$ \textit{edge sampling matrix} given by:
\begin{align*}
    S(e, e) = \frac{\tilde{w_e}}{w_e} = \text{(times e
    sampled)}\frac{w_e}{qp_e}\frac{1}{w_e} = \frac{\text{(times e
    sampled)}}{qp_e}
\end{align*}
Now write $\tilde{L}$ in terms of $S$, which controls `how much' of each
edge is preserved:
\begin{align*}
    \tilde{L} = B^T\tilde{W}B = B^TWSB
\end{align*}
In expectation, the sparsifier exactly preserves all edge weights (implying
$E[\tilde{L}] = L$ as desired):
\begin{align*}
    E\left[\frac{\tilde{w_e}}{w_e}\right] = \frac{1}{qp_e}E[\text{(times e
    sampled)}] = \frac{1}{qp_e}qp_e = 1
\end{align*}
Now we prove the main theorem in this section: \\

\noindent
\textlcsc{Theorem 4 (Spielman-Srivastava)}: If $||\Pi S\Pi - \Pi\Pi||_2 \leq
\epsilon$, we have $(1-\epsilon)L \preceq \tilde{L} \preceq (1+\epsilon)L$.

\noindent
By definition of the 2-norm for symmetric matrices:
\begin{align*}
    ||\Pi S\Pi - \Pi\Pi||_2 &= \sup_{y \in \R^m - \{\zero\}} \frac{|y^T(\Pi
    S\Pi - \Pi\Pi)y|}{y^Ty} \\
                            &= \sup_{y \in \R^m - \{\zero\}} \frac{|y^T(\Pi
    (S - I)\Pi)y|}{y^Ty} \\
                            &= \sup_{y \in \im(\Pi)} \frac{|y^T(\Pi
    (S - I)\Pi)y|}{y^Ty}
\end{align*}
In the last step, we only optimize over $\im(\Pi) \subset \text{dom}(\Pi)$
since the norm must be non-negative. Now, since $y \in \im(\Pi)$ already, we
have $\Pi y = y$ so we can drop the extra $\Pi$ terms:
\begin{align*}
    ||\Pi S\Pi - \Pi\Pi||_2 = \sup_{y \in \im(\Pi)} \frac{|y^T(S -
    I)y|}{y^Ty}
\end{align*}
Since we also have $y \in \im(W^{1/2}B)$, we know $y = W^{1/2}Bx$ for some
$x$:
\begin{align*}
    ||\Pi S\Pi - \Pi\Pi||_2 &= \sup_{x \in \R^n, W^{1/2}Bx \not= \zero}
    \frac{|x^TB^TW^{1/2}SW^{1/2}Bx - x^TB^TWBx|}{x^TB^TWBx} \\
    &= \sup_{x \in \R^n, W^{1/2}Bx \not= \zero} \frac{x^T\tilde{L}x -
x^TLx}{x^TLx} \\
    &\leq \epsilon \quad \text{(by assumption)}
\end{align*}
With some rearranging, we get:
\begin{align*}
    \frac{x^TLx - x^T\tilde{L}x}{x^TLx} &\leq \epsilon \\
    x^TLx - x^T\tilde{L}x &\leq \epsilon x^TLx \\
    (1 - \epsilon)x^TLx &\leq x^T\tilde{L}x
\end{align*}
Analogous algebra with $\frac{x^T\tilde{L}x - x^TLx}{x^TLx}$ lets us
conclude $(1-\epsilon)L \preceq \tilde{L} \preceq (1+\epsilon)L$. We are now
equipped with a powerful link between $L$ and $\Pi$ which simplifies the
remainder of the analysis and leads to a practical edge-sampling strategy.

\subsection{Bounding the expected approximation error}

In this section we take a closer look at edge-selection probabilities and
show how to bound the projection matrix approximation error $||\Pi S \Pi -
\Pi\Pi||^2_2$ w.h.p. We use the following bound: \\

\noindent
\textlcsc{Theorem 5 (Rudelson-Vershynin)}: Let $\hat{p}$ be a probability
distribution over $\Omega \subset \R^d$ s.t $\sup_{y \in \Omega} ||y||_2
\leq M$ and $||E_{\hat{p}}[yy^T]||_2 \leq 1$. If $y_{1..q}$ are drawn
independently from $\hat{p}$, then \cite{TheSurvey}
\cite{SpielmanSrivastava} \cite{RudelsonVershynin}:
\begin{align*}
    E\left[\left| \frac{1}{q} \sum^q_{i=1} y_iy^T_i - E[yy^T] \right|_2
    \right] \leq \min\left(CM\sqrt{\frac{\log q}{q}}, 1\right)
\end{align*}

The probabilities $p_e$ are weight-proportional to the \textit{effective
resistances} $R_e$ from the resistance matrix. Given that $\sum_e w_eR_e =
\Tr(\Pi) = n - 1$, we set $p_e = \frac{w_eR_e}{n - 1}$ to get a normalized
probability distribution. Sampling $q$ edges (with replacement) corresponds
to sampling columns of $\Pi$, giving:
\begin{align*}
    \Pi S\Pi &= \sum_e S(e, e)\Pi_e\Pi^T_e
             = \sum_e \frac{\text{(times e sampled)}}{qp_e}\Pi_e\Pi^T_e
             = \frac{1}{q}\sum_{i=1}^q y_iy^T_i
\end{align*}
Where the $y_i$ are i.i.d from $\hat{y} = \frac{1}{\sqrt{p_e}}\Pi_e$ s.t
$\Pr[\hat{y} = y_e] = p_e$. We now have the first term we need for the
bound. The expected value of $yy^T$ over $\hat{y}$ is $\sum_e
p_e\frac{1}{\sqrt{p_e}}\frac{1}{\sqrt{p_e}}\Pi_e\Pi^T_e = \Pi^2 = \Pi$,
which gives us the second term we need. We verify that $||E[yy^T]||_2 =
||\Pi||_2 \leq 1$ (by definition, the spectral or 2-norm of $\Pi$ is its
largest singular value, 1). Finally, we verify that $y$ has an upper bound:
\begin{align*}
    ||y||_2 = \frac{1}{p_e}\sqrt{\Pi(e, e)} = \sqrt{\frac{n -
    1}{w_eR_e}w_eR_e} = \sqrt{n-1} = M
\end{align*}
We pick $q \in O(\frac{C^2}{\epsilon^2}n\log n)$ and apply Theorem 5 to get:
\begin{align*}
    E[|\Pi S \Pi - \Pi\Pi|_2] =
    E\left[\left| \frac{1}{q} \sum^q_{i=1} y_iy^T_i - E[yy^T] \right|_2
    \right] \leq \frac{\epsilon}{2}
\end{align*}
Which holds for large $n$ and when $\epsilon \geq \frac{1}{\sqrt{n}}$.
Markov's inequality tells us that the probability of the approximation error
being greater than $\epsilon$ is upper-bounded by $E[|\Pi S \Pi -
\Pi\Pi|_2]/\epsilon = \frac{1}{2}$. In practice we expect the approximation
error to be tightly concentrated around $\frac{\epsilon}{2}$.

\subsection{Computing the edge selection probabilities}

In this section we discuss how to find a good set of edge selection
probabilities. Recall that $p_e \approx w_eR_e$, and that the effective
resistance matrix is given by $R = BL^+B^T$. It would be slow to compute $R$
exactly.  Fortunately, an $\alpha$-approximation of $R$ can be used in
Theorem 5 with the only side-effect of pushing the expected approximation
error to $\frac{\alpha\epsilon}{2}$.

To find a good approximation of $R$, we use a fact proven in Section 4: the
effective resistances $R_e$ are just pairwise distances of vectors in the
space: \begin{align*}
    W^{1/2}BL^+\one_v \quad \forall{v \in V}
\end{align*}
These distances are more-or-less preserved if we project this space onto a
subspace spanned by $O(\log m)$ random vectors: \\

\noindent
\textlcsc{Theorem 6 (Johnson-Lindenstrauss)}: Fix vectors $v_{1..m} \in
\R^n$. Let $\epsilon > 0$, $k \geq 24\epsilon^{-2}\log m$, and $Q_{k \times
m}$ be a random matrix with entries $\pm k^{-1/2}$. Then with probability
$\geq 1 - \frac{1}{m}$ \cite{SpielmanSrivastava} \cite{Achlioptas}:
\begin{align*}
    (1-\epsilon)||v_i-v_j||^2_2 \leq ||Qv_i-Qv_j||^2_2 \leq
    (1+\epsilon)||v_i-v_j||^2_2 \quad \forall{(i,j)}
\end{align*}

Let $\tilde{Z} \approx QW^{1/2}BL^+$. We can use a fast solver for Laplacian
systems to compute its rows. Spielman-Srivastava use the solver from an
earlier paper \cite{SpielmanTeng}. A later survey recommends the solver by
Koutis et. al \cite{TheSurvey} \cite{FastLaplacianSolver}.

\subsection{Overview}

In this section we summarize the Spielman-Srivastava algorithm. We also
mention one last spectral sparsification algorithm. It produces a sparsifier
with a linear number of edges in cubic time.  Standard Spielman-Srivastava
can be run as follows:
\begin{itemize}
    \item Compute an approximate effective resistance matrix $\tilde{Z}
        \approx QW^{1/2}BL^+$ by using a fast solver for Laplacian systems.

        The entries of the matrix $Q$ are $\pm k^{-1/2}$, where the sign is
        positive with probability $1/2$. We assume efficient representations
        for $W$ and $B$ are used, since they contain only $m$ and $2m$
        entries respectively. It is not necessary to compute $\text{SVD}(L)$
        to find $L^+$: we assume the Laplacian solver computes the
        pseudo-inverse efficiently. Each one of the $O(\log m)$ rows of
        $\tilde{Z}$ can be computed by running $\tilde{z}_i =
        \text{LSolve}(L, [QW^{1/2}B]_i, \delta)$ for some error parameter
        $\delta$.

    \item Compute $p_e = \frac{w_e||\tilde{Z}(\one_u -
        \one_v)||^2_2}{n-1}$ for each edge. Sample $q$ edges based on the
        distribution $p_e$.

        Note that edges may be sampled efficiently using the array $P =
        [p_{e_1}, p_{e_1} + p_{e_2}, ..., \sum_e p_e]$. Once $P$ is
        constructed, we can sample an edge by generating a random number $r
        \in [0,1]$ and binary searching for $r$ in $P$. 
\end{itemize}
There is another sparsification algorithm by Batson, Spielman, and
Srivastava which produces $O(n)$-edge spectral sparsifiers in $O(mn^3)$ time
\cite{BatsonSpielmanSrivastava}. We mention it here for two reasons.  First,
it exhibits a classic space-time tradeoff (i.e it has an asymptotically
higher runtime but produces sparser results). Second, it implies that every
graph has a high-quality spectral sparsifier with a linear number of edges,
which is very surprising.

\section{Computing Electrical Flows}

In this section we explain why $R$ is called the effective resistance
matrix, and show how to use $\tilde{Z}$ to approximate the resistance
between any pair of vertices in a circuit. In conjunction with Ohm's law,
this information gives us a way to compute voltage drops between nodes in a
circuit as well as current flow.

Consider a graph $G$ where the edges represent wires and the vertices
represent nodes in a circuit. The edge weights correspond to conductances
(i.e inverse resistances). Following the convention in
\cite{SpielmanSrivastava}, let $i_{ext}(u)$ denote the amount of
externally-supplied current at each node, let $i(e)$ denote the current flow
along each edge, and let $v(u)$ denote the voltage at each node. The current
flowing into a node equals the current flowing out:
\begin{align*}
    B^Ti = i_{ext}
\end{align*}
By Ohm's law, $i = v/r = v\sigma$, so:
\begin{align*}
    i = WBv
\end{align*}
This gives $i_{ext} = B^T(WBv) = Lv$. As long as we don't have a shorted
circuit, $i_{ext}$ is not in $\ker(L)$ and we may write $v = L^+i_{ext}$. To
compute effective resistances between any two nodes $(u, v)$, we inject a
unit current at $u$ and extract the current at $v$:
\begin{align*}
    v_v - v_u = (\one_v - \one_u)^Tv
\end{align*}
Because $i_{ext} = (\one_v - \one_u)$, this expression gives us a row of
$B^TL^+B = R$, the matrix of effective resistances between all pairs of
nodes in the circuit. By the work done in the previous section, we know that
it is possible to compute $\tilde{Z}$ in $\tilde{O}(m)$ time. We can
approximate effective resistances between any pair of vertices by taking
$||\tilde{Z}(\one_u -  \one_v)||^2_2$, which is fast ($O(\log m)$ time).

\bibliographystyle{unsrt}
\bibliography{ref}

\end{document}
