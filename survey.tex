\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage[a4paper]{geometry}

\title{Spectral Graph Sparsification}
\author{\small{Vedant Kumar -- \texttt{vsk@berkeley.edu} -- CS270 Survey}}

\begin{document}
\maketitle

\newcommand \cut[1]{\text{cut}_{#1}}
\newcommand \textlcsc[1]{\textsc{\MakeLowercase{#1}}}
\newcommand \Tr{\text{Tr}}

\section*{Abstract}

Graph sparsification algorithms find sparse approximations of graphs. In
this paper we survey major results in spectral sparsification, a special
case in which important structural and algebraic properties of graphs are
preserved. We explain a key result: that every graph has a spectral
sparsifier with $O(|V|\log|V|)$ edges w.h.p, and that such sparsifiers can
be found in $\tilde{O}(|E|)$ time. We finish by discussing some applications
of spectral sparsification, such as computing electrical flows and finding
min-cuts in nearly-linear time.

\section{Introduction}

Sparse graphs have roughly the same number of edges and vertices, up to a
polylog factor ($|E| \in \tilde{O}(|V|)$). In contrast, dense graphs (such
as the complete graph) permit a quadratic number of edges ($|E| \in
O(|V|^2)$). If the number of vertices is fixed, many graph algorithms run
significantly faster and with lower storage requirements on sparse inputs.
Moreover, each edge in a sparse graph tends to contribute `more' to the
overall shape and structure of the graph.  Working with sparse graphs can
save time, save space, and provide insight into the nature of a graph: this
makes graph sparsification interesting.

In order to assess the quality of a sparsification algorithm, we need a way
to measure similarity between a graph and its sparse approximation. Section
2 discusses two different measures of graph similarity -- \textit{cut
similarity} and \textit{spectral similarity} -- both of which led to the
discovery of important sparsification algorithms. Section 3 contains a
detailed exposition of a fast spectral sparsification algorithm based on an
edge-sampling scheme. Section 4 mentions some applications.

\section{Graph Similarity}

In this section we explain how \textit{cut similarity} naturally leads to
\textit{spectral similarity}, a strictly stronger measure of sparsifier
quality. We also mention influential results in sparsification.

\subsection{Conventions}

The only graphs we consider are undirected, weighted, and connected. A graph
can be described by a tuple $G = (V, E, w)$, where $w : V^2 \rightarrow \Re$
maps edges to their weights. All edges must have positive weights, so $w(u,
v) = 0 \Leftrightarrow (u, v) \not\in E$, allowing us to write $G = (V, w)$
w.l.o.g. For notational simplicity we define $n = |V|$ and $m = |E|$. When
comparing two graphs, we always assume that they share the same vertex set.
Finally, we use the notation $A_k$ to refer to the $k$-th column vector of
some matrix $A$. 

\subsection{Cut similarity}

In this section we describe cut similarity and cite an early result in the
field. Two graphs are \textit{cut similar} if all of their cuts have
approximately similar weights. To make this concrete, consider two graphs $G
= (V, w)$ and $\tilde{G} = (V, \tilde{w})$. A \textit{cut} is a subset of
vertices $S \subset V$.  The weight of a cut is given by the sum of the
weights of edges on the cut:
\begin{align*}
    \cut{G}(S) = \sum_{u \in S, v \in V - S} w(u, v)
\end{align*}
$G$ and $\tilde{G}$ are $\epsilon$-cut similar if for all possible cuts, we
have:
\begin{align*}
    (1 - \epsilon)\cut{\tilde{G}}(S) \leq \cut{G}(S) \leq (1 +
    \epsilon)\cut{\tilde{G}}(S) \quad \forall{S \subset V}
\end{align*}
An important early result states that every graph has a good cut similar
approximation with $\tilde{O}(n)$ edges, and that this approximation can be
found quickly: \\

\noindent
\textlcsc{Theorem 1 (Bencz\'{u}r-Karger)}: \textit{Let $\epsilon > 0$. $G =
(V, E)$ has an $\epsilon$-cut similar subgraph $\tilde{G}$ s.t $\tilde{m}
\in O(\epsilon^{-2}n\log n)$. $\tilde{G}$ can be found in $O(m\log^3n +
\epsilon^{-2}m\log n)$ time \cite{BenczurKarger}.} \\

This version of Theorem 1 is taken from a later survey \cite{TheSurvey}. The
original paper claims that an even better $O(m\log^2 n)$ time bound is
possible using the union-find data structure. Regardless, this result sets
the stage for future work by showing that high-quality cut-similar
sparsifiers exist for \textit{every} graph. 

\subsection{Spectral similarity}

In this section we motivate and define spectral similarity. The
\textit{Laplacian quadratic form} of a graph $G = (V, E, w)$ is given by the
map $Q_G : \Re^n \rightarrow \Re$:
\begin{align*}
    Q_G(x) = \sum_{(u, v) \in E} w(u, v)(x(u) - x(v))^2
\end{align*}
Consider a cut $S \subset V$: the $i$-th component of its
\textit{characteristic vector} $\mathbf{1}_S$ is 1 if $v_i \in S$, and is 0
otherwise. The expression $(\mathbf{1}_S(u) - \mathbf{1}_S(v))^2$ is 1 iff
$(u, v)$ is an edge on the cut boundary of $S$, and is 0 otherwise. We
conclude that $Q_G(\mathbf{1}_S)$ sums the weights of the edges on the cut
$S$:
\begin{align*}
    \cut{G}(S) = Q_G(\mathbf{1}_S)
\end{align*}
We get cut similarity by restricting $Q_G$ to $\{0, 1\}^n$. The graphs $(G,
\tilde{G})$ are \textit{$\epsilon$-spectrally similar} if:
\begin{align*}
    (1 - \epsilon)Q_{\tilde{G}}(x) \leq Q_G(x) \leq (1 +
    \epsilon)Q_{\tilde{G}}(x) \quad \forall{x \in \Re^n}
\end{align*}

There is an intuitive way to see that cut similarity is not as strong as
spectral similarity. Consider a $k$-cycle and a $k$-path. We can make the
$k$-path a $k$-cycle by adding an edge $e'$. Most cuts on $\{1..k\}$ have
identical weights on both graphs because they don't touch $e'$, so the
graphs are fairly cut-similar. However, we know that the $k$-cycle is
2-connected while the $k$-path isn't. We conclude that the second-smallest
eigenvalues of the two graphs are dissimilar, implying spectral
dissimilarity \cite{Fiedler}. Apart from connectivity, spectrally similar
graphs are more likely to share a host of other structural characteristics
captured by Laplacian quadratic form.

Spectrally similar graphs also share algebraic properties. Consider the
problem of finding vertex labellings $(l, \tilde{l})$ for
$\epsilon$-spectrally similar graphs $(G, \tilde{G})$ with some of the
labels fixed a priori. We can pose this as a regression problem. First solve
for the unknowns in $l \in \Re^n$ by minimizing $Q_G(l)$, then use the
result to approximate a full labelling $\tilde{l}$. Intuitively, we expect
to find a good approximation because $Q_G(x)$ and $Q_{\tilde{G}}(x)$ match
very well over $\Re^n$ \cite{TheSurvey}.

\subsection{The Laplacian quadratic form}

In this section we explain some important properties of the Laplacian
quadratic form. We rely on these properties heavily throughout the rest of
this survey. The central object we are interested in is the $n \times n$
graph \textit{Laplacian matrix}:
\begin{align*}
    L_G &= D - A \\
        &= 
    \begin{cases}
        -w(u, v) \quad &\text{if } u \not= v \\
        \sum_{z} w(u, z) \quad &\text{if } u = v
    \end{cases}
\end{align*}
Here, $D$ is a diagonal matrix of vertex degrees and $A$ is the adjacency
matrix.  We can use $L_G$ to simplify the Laplacian quadratic form:
\begin{align*}
    Q_G(x) = x^TL_Gx
\end{align*}
To see why this is true, recall that the quadratic form of a matrix $A$ is
$x^TAx = \sum_{i,j} A_{i,j}x_ix_j$.  Every edge $(u, v)$ in $G$ contributes
$w(u, v)[x_u^2 - 2x_ux_v + x_v^2]$ to the sum $Q_G(x)$. If we add up the
following contributions, it becomes clear that our two definitions are
equivalent:
\begin{align*}
    i = u, j = u &:\quad w(u, v)x_u^2 \\
    i = v, j = v &:\quad w(u, v)x_v^2 \\
    i = u, j = v &:\quad -w(u, v)x_ux_v \\
    i = v, j = u &:\quad -w(u, v)x_ux_v
\end{align*}
We can order symmetric PSD matrices by looking at their quadratic forms:
\begin{align*}
    A \preceq B \Leftrightarrow x^TAx \leq x^TBx \quad \forall{x \in \Re^n}
\end{align*}
Now we can check if two graphs are $\epsilon$-spectrally similar by
comparing their Laplacians \footnote{We have used the fact that the
Laplacian is positive semi-definite here, which is shown in Section 3.}:
\begin{align*}
    (1 - \epsilon)L_{\tilde{G}} \preceq L_G \preceq (1 +
    \epsilon)L_{\tilde{G}}
\end{align*}
The `$\preceq$' relation would be more useful if there were a simple way to
compute it. By the Courant-Fischer theorem \cite{CourantFischer}, the $i$-th
eigenvalue of a matrix is given by:
\begin{align*}
    \lambda_i(A) = \max_{S: \dim(S) = i} \min_{x \in S} \frac{x^TAx}{x^Tx}
\end{align*}
Note that the quadratic form of $A$ appears in $\lambda_i(A)$. We conclude
that for $\epsilon$-spectrally similar graphs, the eigenvalues of the graph
Laplacians are similar:
\begin{align*}
    (1 - \epsilon)\lambda_i(L_{\tilde{G}}) \leq \lambda_i(L_G) \leq (1 +
    \epsilon)\lambda_i(L_{\tilde{G}}) \quad \forall{i}
\end{align*}
This fact can be used to show that the total edge weight of a graph is
roughly preserved by an $\epsilon$-spectral approximation:
\begin{align*}
    \Tr(L_G) = 2\sum_{e \in E} w(e) = \sum_i \lambda_i(L_G) \approx
    \Tr(L_{\tilde{G}}) = 2\sum_{e \in \tilde{E}} \tilde{w}(e)
\end{align*}
Edges in spectral sparsifiers literally `carry more weight' and play a
greater role in maintaining the structure of the graph.

\subsection{Building spectral sparsifiers}

In this section, we discuss a result which shows that every graph has a
sparse, spectrally-similar cousin. We are interested in building spectral
sparsifiers using edge-sampling s.t $\tilde{G} \subset G$. In order to
achieve spectral similarity, we would like $L_{\tilde{G}}$ to be close to
$L_G$ in expectation. We can enforce $E[L_{\tilde{G}}] = L_G$ with the
following process:
\begin{itemize}
    \item Assign each edge a selection probability $p_e$ (most of our effort
        is expended here).
    \item Initialize all edges weights $\tilde{w}_e$ in $\tilde{G}$ to 0:
        when an edge $e$ is added to $\tilde{G}$, add $w_e/p_e$ to
        $\tilde{w}_e$.
    \item Sample edges from $G$ until $\tilde{G}$ is large enough.
\end{itemize}

If the edge selection probabilities are too high, $\tilde{G}$ becomes too
dense. If they are too low, the spectrum of $L_{\tilde{G}}$ may not be close
enough to the spectrum of $L_G$. The probabilities $p_e$ should reflect the
fact that some edges are more important than others.

Informally, the \textit{conductance} of a subset $S \subset V$ is the number
of edges on the cut boundary of $S$ divided by the number of edges
`contained' by the cut.  An early edge-sampling scheme worked by splitting
up the vertex set of $G$ into high-conductance components, and by
prioritizing the edges between these components. This led to the following
result: \\

\noindent
\textlcsc{Theorem 2 (Spielman-Teng)}: \textit{Let $\epsilon > 0$. W.h.p $G =
(V, E)$ has an $\epsilon$-spectrally similar subgraph $\tilde{G}$ s.t
$\tilde{m} \in O(\epsilon^{-2}n\log^2 n)$. $\tilde{G}$ can be found in
$O(m\log^{O(1)}m)$ time \cite{SpielmanTeng} \cite{TheSurvey}.} \\

Theorem 2 implies the somewhat surprising fact that arbitrary graphs have
high-quality spectral sparsifiers. From here, our focus shifts to finding
improved time and space bounds for the sparsification algorithm.

\section{Fast Spectral Sparsification}

The goal of this section is to examine the following fast spectral
sparsification algorithm in detail: \\

\noindent
\textlcsc{Theorem 3 (Spielman-Srivastava)}: \textit{Let $\epsilon > 0$.
W.h.p $G = (V, E)$ has an $\epsilon$-spectrally similar subgraph $\tilde{G}$
s.t $\tilde{m} \in O(\epsilon^{-2}n\log n)$. $\tilde{G}$ can be found in
$\tilde{O}(m)$ time \cite{SpielmanSrivastava} \cite{TheSurvey}.} \\

The Spielman-Srivastava algorithm has two stages: (1) compute edge selection
probabilities, and (2) sample edges. The first step calls for the solution
of $\log m$ Laplacian systems, for which the best known time bound is $O(m
\log m \log n \log\log n)$ \cite{FastLaplacianSolver} \cite{TheSurvey}. The
second step can be completed in $O(\epsilon^{-2}n\log n)$ time, for a total
runtime of $\tilde{O}(m)$.

\subsection{Preliminaries}

We need some basic facts and definitions to explain Theorem 3. Let each edge
in $G$ have some fixed (possibly arbitrary) orientation. The $(m \times n)$
\textit{signed edge-vertex incidence matrix} is:
\begin{align*}
    B(e, v) =
    \begin{cases}
        1 \quad &\text{if } e = (v, u) \\
        -1 \quad &\text{if } e = (u, v) \\
        0 \quad &\text{otherwise}
    \end{cases}
\end{align*}
The diagonal $(m \times m)$ \textit{edge weight matrix} is: $W(e, e) =
w(e)$. The Laplacian matrix $L_G$ can be written in terms of edge weights
and edge-vertex incidences:
\begin{align*}
    L_G = B^TWB
\end{align*}
To see why this is true, consider the 3-cycle (a triangle) where all edge
weights are 1. We have:
\begin{align*}
    B^T =
    \begin{pmatrix}
        1 & 0 & -1 \\
        -1 & 1 & 0 \\
        0 & -1 & 1
    \end{pmatrix} \quad
    B =
    \begin{pmatrix}
        1 & -1 & 0 \\
        0 & 1 & -1 \\
        -1 & 0 & 1
    \end{pmatrix} \quad
    L_G =
    \begin{pmatrix}
        2 & -1 & -1 \\
        -1 & 2 & -1 \\
        -1 & -1 & 2
    \end{pmatrix} \quad
\end{align*}
The $i$-th diagonal element of $B^TB$ is given by $B^T_iB_i$, which simply
counts the number of edges in the neighborhood of vertex $i$ (i.e $\sum_z
w(i, z)$). For other entries $B^T_jB_i$ ($i \not= j$), all non-zero terms in
the inner product must be negative because they must correspond to one of
$(i, j)$ or $(j, i)$ (i.e $-w(i, j)$). The result is exactly the graph
Laplacian.  If this intuitive approach is not satisfactory, it may suffice
to verify that $L_G = B^T\mathbf{1}B$ for the triangle.

We can now easily verify that $L_G$ is positive semi-definite (PSD):
\begin{align*}
    x^TL_Gx = x^TB^TWBx = ||W^{\frac{1}{2}}Bx||^2_2 \geq 0 \quad \forall{x
    \in \Re^n}
\end{align*}

\begin{enumerate}[1.]
    \item $L_G$ is symmetric.

        Proof: For the off-diagonal elements $u \not= v$, we have: $L_G(u,
        v) = L_G(v, u) = -w(u, v)$.

    \item $L_G$ 
\end{enumerate}

Cover the effective-resistances paper.

\section{Applications}

\bibliographystyle{unsrt}
\bibliography{ref}

\begin{enumerate}[1.]
    \item Achlioptas, D., Mcsherry, F. Fast computation of low-rank matrix
        approximations. 2 (2007), 9.

    \item Batson, J.D., Spielman, D.A., Srivastava, N.  Twice-Ramanujan
        sparsifiers. 6 (2012), 1704 –1721.

    \item Chandra, A.K., Raghavan, P., Ruzzo, W.L., Smolensky, R., Tiwari,
        P.  The electrical resistance of a graph captures its commute and
        cover times. In (1989), ACM, New York, NY, USA, 574–586.
\end{enumerate}

\end{document}
