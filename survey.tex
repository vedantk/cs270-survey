\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage[a4paper]{geometry}

\title{Spectral Graph Sparsification}
\author{\small{Vedant Kumar -- \texttt{vsk@berkeley.edu} -- CS270 Survey}}

\begin{document}
\maketitle

\newcommand \R{\mathbb{R}}
\newcommand \one{\mathbf{1}}
\newcommand \zero{\mathbf{0}}
\newcommand \Tr{\text{Tr}}
\newcommand \im{\text{im}}
\newcommand \Span{\text{span}}
\newcommand \cut[1]{\text{cut}_{#1}}
\newcommand \textlcsc[1]{\textsc{\MakeLowercase{#1}}}

\section*{Abstract}

Graph sparsification algorithms find sparse approximations of graphs. In
this paper we survey major results in spectral sparsification, a special
case in which important structural and algebraic properties of graphs are
preserved. We explain a key result: that every graph has a spectral
sparsifier with $O(|V|\log|V|)$ edges w.h.p, and that such sparsifiers can
be found in $\tilde{O}(|E|)$ time. We finish by discussing some applications
of spectral sparsification, such as computing electrical flows and finding
min-cuts in nearly-linear time.

\section{Introduction}

Sparse graphs have roughly the same number of edges and vertices, up to a
polylog factor ($|E| \in \tilde{O}(|V|)$). In contrast, dense graphs (such
as the complete graph) permit a quadratic number of edges ($|E| \in
O(|V|^2)$). If the number of vertices is fixed, many graph algorithms run
significantly faster and with lower storage requirements on sparse inputs.
Moreover, each edge in a sparse graph tends to contribute `more' to the
overall shape and structure of the graph.  Working with sparse graphs can
save time, save space, and provide insight into the nature of a graph: this
makes graph sparsification interesting.

In order to evaluate sparsification algorithms, we need to measure the
similarity between a graph and its sparse approximation. Section 2 discusses
two different measures of graph similarity -- \textit{cut similarity} and
\textit{spectral similarity} -- both of which led to the discovery of
important sparsification algorithms. Section 3 contains a detailed
exposition of a fast spectral sparsification algorithm based on an
edge-sampling scheme. Section 4 mentions some applications.

\section{Graph Similarity}

In this section we explain how \textit{cut similarity} naturally leads to
\textit{spectral similarity}, a strictly stronger measure of sparsifier
quality. We also mention influential results in sparsification.

\subsection{Conventions}

The only graphs we consider are undirected, weighted, and connected. A graph
can be described by a tuple $G = (V, E, w)$, where $w : V^2 \rightarrow \R$
maps edges to their weights. All edges must have positive weights, so $w(u,
v) = 0 \Leftrightarrow (u, v) \not\in E$, allowing us to write $G = (V, w)$
w.l.o.g. For notational simplicity we define $n = |V|$ and $m = |E|$. When
comparing two graphs, we always assume that they share the same vertex set.
Finally, we use the notation $A_k$ to refer to the $k$-th column vector of
some matrix $A$. 

\subsection{Cut similarity}

In this section we describe cut similarity and cite an early result in the
field. Two graphs are \textit{cut similar} if all of their cuts have
approximately similar weights. To make this concrete, consider two graphs $G
= (V, w)$ and $\tilde{G} = (V, \tilde{w})$. A \textit{cut} is a subset of
vertices $S \subset V$.  The weight of a cut is given by the sum of the
weights of edges on the cut:
\begin{align*}
    \cut{G}(S) = \sum_{u \in S, v \in V - S} w(u, v)
\end{align*}
$G$ and $\tilde{G}$ are $\epsilon$-cut similar if for all possible cuts, we
have:
\begin{align*}
    (1 - \epsilon)\cut{\tilde{G}}(S) \leq \cut{G}(S) \leq (1 +
    \epsilon)\cut{\tilde{G}}(S) \quad \forall{S \subset V}
\end{align*}
An important early result states that every graph has a good cut similar
approximation with $\tilde{O}(n)$ edges, and that this approximation can be
found quickly: \\

\noindent
\textlcsc{Theorem 1 (Bencz\'{u}r-Karger)}: \textit{Let $\epsilon > 0$. $G =
(V, E)$ has an $\epsilon$-cut similar subgraph $\tilde{G}$ s.t $\tilde{m}
\in O(\epsilon^{-2}n\log n)$. $\tilde{G}$ can be found in $O(m\log^3n +
\epsilon^{-2}m\log n)$ time \cite{BenczurKarger}.} \\

This version of Theorem 1 is taken from a later survey \cite{TheSurvey}. The
original paper claims that an even better $O(m\log^2 n)$ time bound is
possible using the union-find data structure. Regardless, this result sets
the stage for future work by showing that high-quality cut-similar
sparsifiers exist for \textit{every} graph. 

\subsection{Spectral similarity}

In this section we motivate and define spectral similarity. The
\textit{Laplacian quadratic form} of a graph $G = (V, E, w)$ is given by the
map $Q_G : \R^n \rightarrow \R$:
\begin{align*}
    Q_G(x) = \sum_{(u, v) \in E} w(u, v)(x(u) - x(v))^2
\end{align*}
Consider a cut $S \subset V$: the $i$-th component of its
\textit{characteristic vector} $\one_S$ is 1 if $v_i \in S$, and is 0
otherwise. The expression $(\one_S(u) - \one_S(v))^2$ is 1 iff $(u, v)$ is
an edge on the cut boundary of $S$, and is 0 otherwise. We conclude that
$Q_G(\one_S)$ sums the weights of the edges on the cut $S$:
\begin{align*}
    \cut{G}(S) = Q_G(\one_S)
\end{align*}
We get cut similarity by restricting $Q_G$ to $\{0, 1\}^n$. The graphs $(G,
\tilde{G})$ are \textit{$\epsilon$-spectrally similar} if:
\begin{align*}
    (1 - \epsilon)Q_{\tilde{G}}(x) \leq Q_G(x) \leq (1 +
    \epsilon)Q_{\tilde{G}}(x) \quad \forall{x \in \R^n}
\end{align*}

There is an intuitive way to see that cut similarity is not as strong as
spectral similarity. Consider a $k$-cycle and a $k$-path. We can make the
$k$-path a $k$-cycle by adding an edge $e'$. Most cuts on $\{1..k\}$ have
identical weights on both graphs because they don't touch $e'$, so the
graphs are fairly cut-similar. However, we know that the $k$-cycle is
2-connected while the $k$-path isn't. We conclude that the second-smallest
eigenvalues of the two graphs are dissimilar, implying spectral
dissimilarity \cite{Fiedler}. Apart from connectivity, spectrally similar
graphs are more likely to share a host of other structural characteristics
captured by Laplacian quadratic form.

Spectrally similar graphs also share algebraic properties. Consider the
problem of finding vertex labellings $(l, \tilde{l})$ for
$\epsilon$-spectrally similar graphs $(G, \tilde{G})$ with some of the
labels fixed a priori. We can pose this as a regression problem. First solve
for the unknowns in $l \in \R^n$ by minimizing $Q_G(l)$, then use the
result to approximate a full labelling $\tilde{l}$. Intuitively, we expect
to find a good approximation because $Q_G(x)$ and $Q_{\tilde{G}}(x)$ match
very well over $\R^n$ \cite{TheSurvey}.

\subsection{The Laplacian quadratic form}

In this section we explain some important properties of the Laplacian
quadratic form. We rely on these properties heavily throughout the rest of
this survey. The central object we are interested in is the $n \times n$
graph \textit{Laplacian matrix}:
\begin{align*}
    L_G &= D - A \\
        &= 
    \begin{cases}
        -w(u, v) \quad &\text{if } u \not= v \\
        \sum_{z} w(u, z) \quad &\text{if } u = v
    \end{cases}
\end{align*}
Here, $D$ is a diagonal matrix of vertex degrees and $A$ is the adjacency
matrix.  We can use $L_G$ to simplify the Laplacian quadratic form:
\begin{align*}
    Q_G(x) = x^TL_Gx
\end{align*}
To see why this is true, recall that the quadratic form of a matrix $A$ is
$x^TAx = \sum_{i,j} A_{i,j}x_ix_j$.  Every edge $(u, v)$ in $G$ contributes
$w(u, v)[x_u^2 - 2x_ux_v + x_v^2]$ to the sum $Q_G(x)$. If we add up the
following contributions, it becomes clear that our two definitions are
equivalent:
\begin{align*}
    i = u, j = u &:\quad w(u, v)x_u^2 \\
    i = v, j = v &:\quad w(u, v)x_v^2 \\
    i = u, j = v &:\quad -w(u, v)x_ux_v \\
    i = v, j = u &:\quad -w(u, v)x_ux_v
\end{align*}
We can order symmetric PSD matrices by looking at their quadratic forms:
\begin{align*}
    A \preceq B \Leftrightarrow x^TAx \leq x^TBx \quad \forall{x \in \R^n}
\end{align*}
Now we can check if two graphs are $\epsilon$-spectrally similar by
comparing their Laplacians \footnote{We have used the fact that the
Laplacian is positive semi-definite here, which is shown in Section 3.}:
\begin{align*}
    (1 - \epsilon)L_{\tilde{G}} \preceq L_G \preceq (1 +
    \epsilon)L_{\tilde{G}}
\end{align*}
The `$\preceq$' relation would be more useful if there were a simple way to
compute it. By the Courant-Fischer theorem \cite{CourantFischer}, the $i$-th
eigenvalue of a matrix is given by:
\begin{align*}
    \lambda_i(A) = \max_{S: \dim(S) = i} \min_{x \in S} \frac{x^TAx}{x^Tx}
\end{align*}
Note that the quadratic form of $A$ appears in $\lambda_i(A)$. We conclude
that for $\epsilon$-spectrally similar graphs, the eigenvalues of the graph
Laplacians are similar:
\begin{align*}
    (1 - \epsilon)\lambda_i(L_{\tilde{G}}) \leq \lambda_i(L_G) \leq (1 +
    \epsilon)\lambda_i(L_{\tilde{G}}) \quad \forall{i}
\end{align*}
This fact can be used to show that the total edge weight of a graph is
roughly preserved by an $\epsilon$-spectral approximation:
\begin{align*}
    \Tr(L_G) = 2\sum_{e \in E} w(e) = \sum_i \lambda_i(L_G) \approx
    \Tr(L_{\tilde{G}}) = 2\sum_{e \in \tilde{E}} \tilde{w}(e)
\end{align*}
Edges in spectral sparsifiers literally `carry more weight' and play a
greater role in maintaining the structure of the graph.

\subsection{Building spectral sparsifiers}

In this section, we discuss a result which shows that every graph has a
sparse, spectrally-similar cousin. We are interested in building spectral
sparsifiers using edge-sampling s.t $\tilde{G} \subset G$. In order to
achieve spectral similarity, we would like $L_{\tilde{G}}$ to be close to
$L_G$ in expectation. We can enforce $E[L_{\tilde{G}}] = L_G$ with the
following process:
\begin{itemize}
    \item Assign each edge a selection probability $p_e$ (most of our effort
        is expended here).
    \item Initialize all edges weights $\tilde{w}_e$ in $\tilde{G}$ to 0:
        when an edge $e$ is added to $\tilde{G}$, add $w_e/p_e$ to
        $\tilde{w}_e$.
    \item Sample edges from $G$ until $\tilde{G}$ is large enough.
\end{itemize}

If the edge selection probabilities are too high, $\tilde{G}$ becomes too
dense. If they are too low, the spectrum of $L_{\tilde{G}}$ may not be close
enough to the spectrum of $L_G$. The probabilities $p_e$ should reflect the
fact that some edges are more important than others.

Informally, the \textit{conductance} of a subset $S \subset V$ is the number
of edges on the cut boundary of $S$ divided by the number of edges
`contained' by the cut.  An early edge-sampling scheme worked by splitting
up the vertex set of $G$ into high-conductance components, and by
prioritizing the edges between these components. This led to the following
result: \\

\noindent
\textlcsc{Theorem 2 (Spielman-Teng)}: \textit{Let $\epsilon > 0$. W.h.p $G =
(V, E)$ has an $\epsilon$-spectrally similar subgraph $\tilde{G}$ s.t
$\tilde{m} \in O(\epsilon^{-2}n\log^2 n)$. $\tilde{G}$ can be found in
$O(m\log^{O(1)}m)$ time \cite{SpielmanTeng} \cite{TheSurvey}.} \\

Theorem 2 implies the somewhat surprising fact that arbitrary graphs have
high-quality spectral sparsifiers. From here, our focus shifts to finding
improved time and space bounds for the sparsification algorithm.

\section{Fast Spectral Sparsification}

The goal of this section is to examine the following fast spectral
sparsification algorithm in detail: \\

\noindent
\textlcsc{Theorem 3 (Spielman-Srivastava)}: \textit{Let $\epsilon > 0$.
W.h.p $G = (V, E)$ has an $\epsilon$-spectrally similar subgraph $\tilde{G}$
s.t $\tilde{m} \in O(\epsilon^{-2}n\log n)$. $\tilde{G}$ can be found in
$\tilde{O}(m)$ time \cite{SpielmanSrivastava} \cite{TheSurvey}.} \\

The Spielman-Srivastava algorithm has two stages: (1) compute edge selection
probabilities, and (2) sample edges. The first step requires solving $\log
m$ Laplacian linear systems, for which the best known time bound is $O(m
\log m \log n \log\log n)$ \cite{FastLaplacianSolver} \cite{TheSurvey}. The
second step can be completed in $O(\epsilon^{-2}n\log n)$ time, for a total
runtime of $\tilde{O}(m)$.

\subsection{Preliminaries}

We need some basic facts and definitions to explain Theorem 3. Let each edge
in $G$ have some fixed (possibly arbitrary) orientation. The $(m \times n)$
\textit{signed edge-vertex incidence matrix} is:
\begin{align*}
    B(e, v) =
    \begin{cases}
        1 \quad &\text{if } e = (v, u) \\
        -1 \quad &\text{if } e = (u, v) \\
        0 \quad &\text{otherwise}
    \end{cases}
\end{align*}
The diagonal $(m \times m)$ \textit{edge weight matrix} is: $W(e, e) =
w(e)$. The Laplacian matrix $L$ can be written in terms of edge weights
and edge-vertex incidences:
\begin{align*}
    L = B^TWB
\end{align*}
To see why this is true, consider the 3-cycle (a triangle) where all edge
weights are 1. We have:
\begin{align*}
    B^T =
    \begin{pmatrix}
        1 & 0 & -1 \\
        -1 & 1 & 0 \\
        0 & -1 & 1
    \end{pmatrix} \quad
    B =
    \begin{pmatrix}
        1 & -1 & 0 \\
        0 & 1 & -1 \\
        -1 & 0 & 1
    \end{pmatrix} \quad
    L = B^TB =
    \begin{pmatrix}
        2 & -1 & -1 \\
        -1 & 2 & -1 \\
        -1 & -1 & 2
    \end{pmatrix}
\end{align*}
The $i$-th diagonal element of $B^TB$ is given by $B^T_iB_i$, which counts
the number of edges in the neighborhood of vertex $i$ (i.e $\sum_z w(i,
z)$). For other entries $B^T_jB_i$ ($i \not= j$), all non-zero terms in the
inner product must be negative because they correspond to one of $(i, j)$ or
$(j, i)$ (i.e $-w(i, j)$). The result is exactly the graph Laplacian.

We now verify that $L$ is positive semi-definite (PSD):
\begin{align*}
    x^TLx = x^TB^TWBx = ||W^{1/2}Bx||^2_2 \geq 0 \quad \forall{x
    \in \R^n}
\end{align*}
The null space of $L$ is the subspace spanned by $\one$. I.e, $\ker(L) =
\ker(W^{1/2}B) = \Span(\one)$:
\begin{align*}
    x^TLx = 0 &\Leftrightarrow ||W^{1/2}Bx||^2_2 = 0 \\
                &\Leftrightarrow x(u) - x(v) = 0 \quad \text{for all edges }
    (u, v) \in E \text{ since } Q_G(x) = 0 \\
    &\Leftrightarrow x \text{ is constant}
\end{align*}
$L$ can be diagonalized using the singular value decomposition. Let
$\lambda_{1..n-1}$ be the non-zero eigenvalues and $u_{1..n-1}$ the
eigenvectors:
\begin{align*}
    L = \sum_{i=1}^{n-1} \lambda_iu_iu^T_i = U\Sigma V^*
\end{align*}
The pseudo-inverse is given by:
\begin{align*}
    L^+ = \sum_{i=1}^{n-1} \frac{1}{\lambda_i}u_iu^T_i = V\Sigma^+ U^*
\end{align*}
When we are restricted to $\im(L) = \Span(\one)^{\bot}$, $LL^+$ is the
identity:
\begin{align*}
    LL^+ &= U\Sigma V^*V\Sigma^+ U^*
             = U\Sigma I \Sigma^+ U^*
             = U I U^*
             = I \quad \text{if } x \not\in \ker(L) \\
    L^+L &= V\Sigma^+ U^*U\Sigma V^*
             = V\Sigma^+ I \Sigma V^*
             = V I V^*
             = I \quad \text{if } x \not\in \ker(L)
\end{align*}
We need the pseudo-inverse because $L$ is not full-rank, and $\zero$ is not
invertible.

\subsection{The Projection matrix}

In this section we define the Projection matrix $\Pi$ and prove some facts
about it. Later, we show that $\Pi$ can be used to bound the spectral
approximation error.  To define $\Pi$, we need the $(m \times m)$ symmetric
\textit{resistance matrix} $R = BL^+B^T$, which has diagonal elements $R(e,
e) = R_e$ \footnote{We explain the significance of $R$ in Section 4.}.  The
$(m \times m)$ \textit{projection matrix} is $\Pi = W^{1/2}RW^{1/2}$. We
list some properties of the projection matrix with proofs:
\begin{enumerate}[1.]
    \item $\Pi$ is a projection matrix.
        
        Proof: $\Pi^2 = \Pi$ can be checked mechanically. In the last step,
        we have $\Pi^2 = W^{1/2}BL^+LL^+B^TW^{1/2} = W^{1/2}BL^+B^TW^{1/2} =
        \Pi$, and we need the fact that $LL^+$ is the identity on
        $\im(L^+)$.

    \item The image of $\Pi$ is equal to the image of $W^{1/2}B$.
        
        Proof: First note that $\im(\Pi) \subseteq \im(W^{1/2}B)$.
        \begin{align*}
            \im(\Pi) = \im(W^{1/2}BL^+B^TW^{1/2}) \subseteq \im(W^{1/2}B)
        \end{align*}
        Any vector $y \in \im(W^{1/2}B)$ must satisfy $W^{1/2}Bx$ for some
        $x \not\in \ker(L)$, which means:
        \begin{align*}
            \Pi y &= W^{1/2}BL^+B^TW^{1/2}W^{1/2}Bx \\
                  &= W^{1/2}BL^+Lx \\
                  &= W^{1/2}Bx \quad \text{since } x \not\in \ker(L) \\
                  &= y \in \im(\Pi)
        \end{align*}
        So $\im(\Pi) = \im(W^{1/2}B)$.

    \item The eigenvalues of $\Pi$ are 1 ($n-1$ copies) and 0 ($m-n+1$
        copies).

        Proof: By rank-nullity on $L$, $\dim(\ker(L)) + \dim(\im(L)) = 1 +
        (n-1) = n$. From this we have $\dim(\im(\Pi)) = n-1$. Because $\Pi$
        is a projection matrix all of its eigenvalues are either 1 or 0:
        since the image of $\Pi$ has dimension $n-1$ it must have $n-1$
        non-zero (`1') eigenvalues. The remaining $m-n+1$ eigenvalues must
        be zeros.

    \item The diagonal elements of $\Pi$ are: $\Pi(e, e) = \sqrt{W(e,
        e)}R_e\sqrt{W(e, e)} = w(e)R_e$. We know $\Pi$ is symmetric by
        symmetry of $W$ and $R$, so we get $\Pi(e, e) = \Pi^2(e, e) =
        \Pi_e^T\Pi_e = ||\Pi_e||^2_2$.
\end{enumerate}

\subsection{Linking the Laplacian and Projection matrices}

Our goal is to find a sparsifier $\tilde{G}$ s.t the quadratic forms of $L$
and $\tilde{L}$ are close. In this section, we show that sparsifiers which
keep the quadratic forms of $\Pi$ and $\tilde{\Pi}$ close achieve this. The
first step towards showing this is finding a symbolic expression for
$\tilde{L}$. Suppose we sample $q$ edges to build $\tilde{G}$. Consider the
random $(m \times m)$ \textit{edge sampling matrix}:
\begin{align*}
    S(e, e) = \frac{\tilde{w_e}}{w_e} = \text{(times e
    sampled)}\frac{w_e}{qp_e}\frac{1}{w_e} = \frac{\text{(times e
    sampled)}}{qp_e}
\end{align*}
Now write $\tilde{L}$ in terms of $S$, which controls `how much' of each
edge is preserved:
\begin{align*}
    \tilde{L} = B^T\tilde{W}B = B^TWSB
\end{align*}
In expectation, the sparsifier exactly preserves all edge weights (implying
$E[\tilde{L}] = L$ as desired):
\begin{align*}
    E\left[\frac{\tilde{w_e}}{w_e}\right] = \frac{1}{qp_e}E[\text{(times e
    sampled)}] = \frac{1}{qp_e}qp_e = 1
\end{align*}
Now we prove the main theorem in this section: \\

\noindent
\textlcsc{Theorem 4 (Spielman-Srivastava)}: If $||\Pi S\Pi - \Pi\Pi||_2 \leq
\epsilon$, we have $(1-\epsilon)L \preceq \tilde{L} \preceq (1+\epsilon)L$.

\noindent
By definition of the 2-norm for symmetric matrices:
\begin{align*}
    ||\Pi S\Pi - \Pi\Pi||_2 &= \sup_{y \in \R^m - \{\zero\}} \frac{|y^T(\Pi
    S\Pi - \Pi\Pi)y|}{y^Ty} \\
                            &= \sup_{y \in \R^m - \{\zero\}} \frac{|y^T(\Pi
    (S - I)\Pi)y|}{y^Ty} \\
                            &= \sup_{y \in \im(\Pi)} \frac{|y^T(\Pi
    (S - I)\Pi)y|}{y^Ty}
\end{align*}
In the last step, we only optimize over $\im(\Pi) \subset \text{dom}(\Pi)$
since the norm must be non-negative. Now, since $y \in \im(\Pi)$ already, we
have $\Pi y = y$ so we can drop the extra $\Pi$ terms:
\begin{align*}
    ||\Pi S\Pi - \Pi\Pi||_2 = \sup_{y \in \im(\Pi)} \frac{|y^T(S -
    I)y|}{y^Ty}
\end{align*}
Since we also have $y \in \im(W^{1/2}B)$, we know $y = W^{1/2}Bx$ for some
$x$:
\begin{align*}
    ||\Pi S\Pi - \Pi\Pi||_2 &= \sup_{x \in \R^n, W^{1/2}Bx \not= \zero}
    \frac{|x^TB^TW^{1/2}SW^{1/2}Bx - x^TB^TWBx|}{x^TB^TWBx} \\
    &= \sup_{x \in \R^n, W^{1/2}Bx \not= \zero} \frac{x^T\tilde{L}x -
x^TLx}{x^TLx} \\
    &\leq \epsilon \quad \text{(by assumption)}
\end{align*}
With some rearranging, we get:
\begin{align*}
    \frac{x^TLx - x^T\tilde{L}x}{x^TLx} &\leq \epsilon \\
    x^TLx - x^T\tilde{L}x &\leq \epsilon x^TLx \\
    (1 - \epsilon)x^TLx &\leq x^T\tilde{L}x
\end{align*}
Analogous algebra gives us for $\frac{x^T\tilde{L}x - x^TLx}{x^TLx}$ lets us
conclude $(1-\epsilon)L \preceq \tilde{L} \preceq (1+\epsilon)L$. We are now
equipped with a powerful link between $L$ and $\Pi$ which simplifies the
remainder of the analysis and leads to a practical edge-sampling strategy.

\subsection{Bounding the expected approximation error}

\subsection{Computing the edge selection probabilities}

\section{Applications}

\bibliographystyle{unsrt}
\bibliography{ref}

\begin{enumerate}[1.]
    \item Achlioptas, D., Mcsherry, F. Fast computation of low-rank matrix
        approximations. 2 (2007), 9.

    \item Batson, J.D., Spielman, D.A., Srivastava, N.  Twice-Ramanujan
        sparsifiers. 6 (2012), 1704 –1721.

    \item Chandra, A.K., Raghavan, P., Ruzzo, W.L., Smolensky, R., Tiwari,
        P.  The electrical resistance of a graph captures its commute and
        cover times. In (1989), ACM, New York, NY, USA, 574–586.
\end{enumerate}

\end{document}
