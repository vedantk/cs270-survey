\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage[a4paper]{geometry}

\title{Spectral Graph Sparsification}
\author{Vedant Kumar}

\begin{document}
\maketitle

\newcommand \cut[1]{\text{cut}_{#1}}
\newcommand \textlcsc[1]{\textsc{\MakeLowercase{#1}}}

\section*{Abstract}

Graph sparsification algorithms find sparse approximations of graphs. In
this paper we survey major results in spectral sparsification, a special
case in which important structural and algebraic properties of graphs are
preserved. We explain and provide intuition for a key result: that every
graph has a spectral sparsifier with $O(|V|\log|V|)$ edges w.h.p, and that
such a sparsifier can be found in $\tilde{O}(|E|)$ time. We finish by
discussing some applications of spectral sparsification, such as computing
electrical flows and finding minimum $s$-$t$ cuts in nearly-linear time.

\section{Introduction}

Sparse graphs have a comparable number of edges and vertices, up to a
poly-logarathmic factor ($|E| \in \tilde{O}(|V|)$). In contrast, dense
graphs (such as the complete graph) permit a quadratic number of edges ($|E|
\in O(|V|^2)$). If the number of vertices is fixed, many graph algorithms
run significantly faster and with lower space requirements on sparse inputs.
Moreover, each edge in a sparse graph tends to contribute `more' to the
overall structure of the graph.  Working with sparse graphs can save time,
save space, and provide insight into the nature of a graph: this makes graph
sparsification interesting.

In order to assess the quality of a sparsification algorithm, we need a way
to measure similarity between a graph and its sparse approximation. Section
2 discusses two different measures of graph similarity --
\textit{cut similarity} and \textit{spectral similarity} -- both of which
led to the discovery of important sparsification algorithms. Section 3
contains a detailed exposition of a fast spectral sparsification algorithm
based on an edge-sampling scheme.  Section 4 mentions some applications of
spectral sparsification, and Section 5 reprises some of the key ideas
presented in this survey.

\section{Graph Similarity}

In this section we explain how \textit{cut similarity} naturally builds up
to \textit{spectral similarity}, a strictly stronger criterion. These
similarity measures ground all further discussion of `good' sparsifiers.

\subsection{Conventions}

The only graphs we will consider are undirected, weighted, and connected. A
graph can be described by a tuple $G = (V, E, w)$, where $w : V^2
\rightarrow \Re$ maps edges to their weights. All edges must have positive
weights, so $w(u, v) \leq 0 \Rightarrow (u, v) \not\in E$, allowing us to
write $G = (V, w)$ w.l.o.g. For notational simplicity we define $n = |V|$
and $m = |E|$. When comparing two graphs, we always assume that they have
the same vertex sets.

\subsection{Cut similarity}

Two graphs are cut similar if all of their cuts have approximately similar
weights. To make this concrete, assume that we have two graphs $G = (V, w)$
and $H = (V, \tilde{w})$. A \textit{cut} is a subset of vertices $S \subset
V$. The weight of a cut is given by the sum of the weights of edges on the
cut:
\begin{align*}
    \cut{G}(S) = \sum_{u \in S, v \in V - S} w(u, v)
\end{align*}
$G$ and $H$ are $\epsilon$-cut similar:
\begin{align*}
    (1 - \epsilon)\cut{H}(S) \leq \cut{G}(S) \leq (1 + \epsilon)\cut{H}(S)
    \quad \forall{S \subset V}
\end{align*}
An important early result states that every graph has a cut similar
approximation with $\tilde{O}(n)$ edges, and that this approximation can be
found quickly: \\

\noindent
\textlcsc{Theorem 1 (Bencz\'{u}r-Karger)}: \textit{Let $\epsilon > 0$. $G =
(V, E)$ has an $\epsilon$-cut similar graph $H = (V, \tilde{E}) \subset G$
s.t $\tilde{m} \in O(\epsilon^{-2}n\log n)$. $H$ can be found in $O(m\log^3n
+ \epsilon^{-2}m\log n)$ time.} \footnote{cite benczur-karger paper} \\

This version of Theorem 1 is taken from a later survey \footnote{cite
survey}. The original paper claims that an even better $O(m\log^2 n)$ time
bound is possible using the union-find data structure. Regardless, this
result sets the stage for future work by showing that high-quality
cut-similar sparsifiers exist for \textit{every} graph. 

\subsection{Spectral similarity}

Def. spectrally similar, discuss.

Describe Spielman-Teng, go up to thm 4.

\section{Fast Sparsification with Sampling}

Cover the effective-resistances paper.

\section{Applications}

\section{Overview}

\section*{References}

\begin{enumerate}[1.]
    \item Achlioptas, D., Mcsherry, F. Fast computation of low-rank matrix
        approximations. 2 (2007), 9.

    \item Batson, J.D., Spielman, D.A., Srivastava, N.  Twice-Ramanujan
        sparsifiers. 6 (2012), 1704 –1721.

    \item Batson, J.D., Spielman, D.A., Srivastava, N, Teng, S.  Spectral
        Sparsification of Graphs: Theory and Applications. 8 (2013),
        Communications of the ACM.

    \item Bencz\'{u}r, A.A., Karger, D.R.  Approximating s-t minimum cuts in
        $O(n^2)$ time. In (1996), 47–55.

    \item Chandra, A.K., Raghavan, P., Ruzzo, W.L., Smolensky, R., Tiwari,
        P.  The electrical resistance of a graph captures its commute and
        cover times. In (1989), ACM, New York, NY, USA, 574–586.

    \item Cheeger, J. A lower bound for smallest eigenvalue of Laplacian. In
        (1970), Princeton University Press, 195–199.

    \item Spielman, D.A., Srivastava, N. Graph sparsification by effective
        resistances.  6 (2011), 1913–1926.
\end{enumerate}

\end{document}
